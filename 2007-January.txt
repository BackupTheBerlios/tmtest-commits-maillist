From tmtest-commits at lists.berlios.de  Tue Jan 23 06:56:27 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Tue, 23 Jan 2007 05:56:27 -0000
Subject: [Tmtest-commits] [116] trunk: Make source code 64-bit friendly.
Message-ID: <200701230555.l0N5t7L0030050@sheep.berlios.de>

Revision: 116
Author:   bronson
Date:     2007-01-23 06:55:05 +0100 (Tue, 23 Jan 2007)

Log Message:
-----------
Make source code 64-bit friendly.

Modified Paths:
--------------
    trunk/re2c/read-fd.c
    trunk/re2c/scan.h
    trunk/test.c
    trunk/tfscan.c
Modified: trunk/re2c/read-fd.c
===================================================================
--- trunk/re2c/read-fd.c	2006-03-08 22:03:36 UTC (rev 115)
+++ trunk/re2c/read-fd.c	2007-01-23 05:55:05 UTC (rev 116)
@@ -33,7 +33,7 @@
 
     // ensure we get a full read
     do {
-        n = read((int)ss->readref, (void*)ss->limit, avail);
+        n = read((long int)ss->readref, (void*)ss->limit, avail);
     } while(n < 0 && errno == EINTR);
     ss->limit += n;
 
@@ -58,7 +58,7 @@
         return 0;
     }
 
-    ss->readref = (void*)fd;
+    ss->readref = (void*)(long int)fd;
     ss->read = readfd_read;
     return ss;
 }
@@ -101,7 +101,7 @@
 
 void readfd_close(scanstate *ss)
 {
-    close((int)ss->readref);
+    close((long int)ss->readref);
     dynscan_free(ss);
 }
 

Modified: trunk/re2c/scan.h
===================================================================
--- trunk/re2c/scan.h	2006-03-08 22:03:36 UTC (rev 115)
+++ trunk/re2c/scan.h	2007-01-23 05:55:05 UTC (rev 116)
@@ -252,6 +252,7 @@
 #define current_token_end(ss) ((ss)->cursor)
 
 /** Returns the length of the most recently scanned token.
+ *  (on all compilers this should be a signed long int)
  */
 
 #define token_length(ss) ((ss)->cursor - (ss)->token)

Modified: trunk/test.c
===================================================================
--- trunk/test.c	2006-03-08 22:03:36 UTC (rev 115)
+++ trunk/test.c	2007-01-23 05:55:05 UTC (rev 116)
@@ -168,7 +168,7 @@
             exit(10);
         } else if(tok == stGARBAGE) {
 			fprintf(stderr, "Garbage on line %d in the status file: '%.*s'\n",
-					ss.line, token_length(&ss)-1, token_start(&ss));
+					ss.line, (int)token_length(&ss)-1, token_start(&ss));
 		} else {
 			state = tok;
 		}
@@ -185,11 +185,11 @@
 						lastfile_good = 1;
 					} else {
 						fprintf(stderr, "CONFIG needs arg on line %d of the status file: '%.*s'\n",
-								ss.line, token_length(&ss)-1, token_start(&ss));
+								ss.line, (int)token_length(&ss)-1, token_start(&ss));
 					}
 				} else {
 					fprintf(stderr, "CONFIG but status (%d) wasn't pending on line %d of the status file: '%.*s'\n",
-							test->status, ss.line, token_length(&ss)-1, token_start(&ss));
+							test->status, ss.line, (int)token_length(&ss)-1, token_start(&ss));
 				}
 				break;
 
@@ -204,11 +204,11 @@
 						lastfile_good = 1;
 					} else {
 						fprintf(stderr, "RUNNING needs arg on line %d of the status file: '%.*s'\n",
-								ss.line, token_length(&ss)-1, token_start(&ss));
+								ss.line, (int)token_length(&ss)-1, token_start(&ss));
 					}
 				} else {
 					fprintf(stderr, "RUNNING but status (%d) wasn't pending on line %d of the status file: '%.*s'\n",
-							test->status, ss.line, token_length(&ss)-1, token_start(&ss));
+							test->status, ss.line, (int)token_length(&ss)-1, token_start(&ss));
 				}
 				break;
 
@@ -217,7 +217,7 @@
 					test->status = test_was_completed;
 				} else {
 					fprintf(stderr, "DONE but status (%d) wasn't RUNNING on line %d of the status file: '%.*s'\n",
-							test->status, ss.line, token_length(&ss)-1, token_start(&ss));
+							test->status, ss.line, (int)token_length(&ss)-1, token_start(&ss));
 				}
 				break;
 			
@@ -233,7 +233,7 @@
 
 			default:
 				fprintf(stderr, "Unknown token (%d) on line %d of the status file: '%.*s'\n",
-						tok, ss.line, token_length(&ss)-1, token_start(&ss));
+						tok, ss.line, (int)token_length(&ss)-1, token_start(&ss));
 		}
     } while(!scan_is_finished(&ss));
 
@@ -494,7 +494,7 @@
         *(int*)refcon = 1;
     } else if(cp < ce) {
         fprintf(stderr, "%s line %d: unknown arguments \"%.*s\"\n",
-                file, line, ce-cp, cp);
+                file, line, (int)(ce-cp), cp);
     }
 
     return 0;

Modified: trunk/tfscan.c
===================================================================
--- trunk/tfscan.c	2006-03-08 22:03:36 UTC (rev 115)
+++ trunk/tfscan.c	2007-01-23 05:55:05 UTC (rev 116)
@@ -34,7 +34,7 @@
 #include "tfscan.h"
 
 
-#define START(x) (ss->scanref=(void*)(x))
+#define START(x) (ss->scanref=(void*)(long int)(x))
 
 #ifndef NULL
 #define NULL ((void*)0)
@@ -93,7 +93,7 @@
 		// We have to assume that we previously read as much data as
 		// possible.  So the entire buffer is just data with no tokens
 		// and no CR/LF.
-		return (int)ss->scanref;
+		return (long int)ss->scanref;
 	}
 
 	if(*YYCURSOR == '\r') YYCURSOR++;
@@ -103,7 +103,7 @@
 
 	// We have potential for finding a token at this point.
 	ss->state = tfscan_tok_start;
-	return (int)ss->scanref;
+	return (long int)ss->scanref;
 }
 
 
@@ -123,7 +123,7 @@
         r = (*ss->read)(ss);
         if(r < 0) return r;
         // if we're at eof, then the current token is just data.
-        if(r == 0) return (int)ss->scanref;
+        if(r == 0) return (long int)ss->scanref;
     }
 
     if(*YYCURSOR != '\r' && *YYCURSOR != '\n' && *YYCURSOR != ':' &&
@@ -142,7 +142,7 @@
             r = (*ss->read)(ss);
             if(r < 0) return r;
             // if we're at eof, then the current token is just data.
-            if(r == 0) return (int)ss->scanref;
+            if(r == 0) return (long int)ss->scanref;
 		}
 	}
 





From tmtest-commits at lists.berlios.de  Tue Jan 23 10:49:29 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Tue, 23 Jan 2007 09:49:29 -0000
Subject: [Tmtest-commits] [117] trunk: Create the TOUCH tmlib function,
	mark MKFILE_EMPTY for
Message-ID: <200701230948.l0N9m4Ag011538@sheep.berlios.de>

Revision: 117
Author:   bronson
Date:     2007-01-23 10:48:03 +0100 (Tue, 23 Jan 2007)

Log Message:
-----------
Create the TOUCH tmlib function, mark MKFILE_EMPTY for deprecation.

Modified Paths:
--------------
    trunk/TODO
    trunk/test/README
    trunk/tmlib.sh
    trunk/tmtest.conf
Modified: trunk/TODO
===================================================================
--- trunk/TODO	2007-01-23 05:55:05 UTC (rev 116)
+++ trunk/TODO	2007-01-23 09:48:03 UTC (rev 117)
@@ -1,4 +1,7 @@
 0.96:
+- MKFILE should just create the file that the user names.
+- MKTMPFILE should create a filename and store that in the user's variable.
+
 - Write unit tests for path normalization.
 - Make tmtest only execute config files owned by either the user or root.
   Print a big fat warning when the config file is skipped.  This prevents
@@ -63,6 +66,7 @@
   it to a file, then cat the file at the end.  We truncate stderr if it
   gets too big.
   - When done, verify that netknife's tests that freeze with -d now pass.
+- get rid of MKFILE_EMPTY, deprecated in favor of TOUCH.
 - Is there any way to record memory and swap usage for each test?
   sure, it's in the rusage. prolly add a "tmtest -v" to print it for each test.
 - Well, piping the result of a command to a function destroys the the

Modified: trunk/test/README
===================================================================
--- trunk/test/README	2007-01-23 05:55:05 UTC (rev 116)
+++ trunk/test/README	2007-01-23 09:48:03 UTC (rev 117)
@@ -1,6 +1,6 @@
 NOTE: If you are looking for examples of testfiles, this is NOT the
-directory for you!  Due to the recursive nature of tmtest testing
-itself, many of these testfiles are very difficult to understand.
+place to go!  Due to the recursive nature of testing tmtest using tmtest,
+many of these testfiles can be quite difficult to understand.
 
 See the examples directory instead.
 

Modified: trunk/tmlib.sh
===================================================================
--- trunk/tmlib.sh	2007-01-23 05:55:05 UTC (rev 116)
+++ trunk/tmlib.sh	2007-01-23 09:48:03 UTC (rev 117)
@@ -144,6 +144,7 @@
 
 #
 # MKFILE_EMPTY
+# TODO: this call is deprecated and will go away.
 #
 # I can't figure out how to get bash to bail instead of blocking.
 # Therefore, if you just want to create an empty file, you either
@@ -156,6 +157,16 @@
 }
 
 
+TOUCH ()
+{
+	while [ "$1" != "" ]; do
+		touch $1
+		ATEXIT "rm '$1'"
+		shift
+	done
+}
+
+
 #
 # MKDIR
 #

Modified: trunk/tmtest.conf
===================================================================
--- trunk/tmtest.conf	2007-01-23 05:55:05 UTC (rev 116)
+++ trunk/tmtest.conf	2007-01-23 09:48:03 UTC (rev 117)
@@ -33,7 +33,7 @@
 set -e
 
 
-# Every test executes tmtest vi the $tmtest variable so that we can
+# Every test executes tmtest via the $tmtest variable so that we can
 # force which binary gets run.  We also provide $tmtest_file to tell
 # the file that contains the executable since $tmtest itself may end
 # up looking like "valgrind tmtest --config..."





From tmtest-commits at lists.berlios.de  Tue Jan 23 11:09:49 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Tue, 23 Jan 2007 10:09:49 -0000
Subject: [Tmtest-commits] [118] trunk/test/03-results: Fix silly mistakes in
	some testfiles.
Message-ID: <200701231008.l0NA8O0V013248@sheep.berlios.de>

Revision: 118
Author:   bronson
Date:     2007-01-23 11:08:23 +0100 (Tue, 23 Jan 2007)

Log Message:
-----------
Fix silly mistakes in some testfiles.

Modified Paths:
--------------
    trunk/test/01-testfile/dumpscript/12-TrailingParentDir.test
    trunk/test/02-running/50-OpenFDsTest.test
    trunk/test/03-results/32-BinaryLarge.test
Modified: trunk/test/01-testfile/dumpscript/12-TrailingParentDir.test
===================================================================
--- trunk/test/01-testfile/dumpscript/12-TrailingParentDir.test	2007-01-23 09:48:03 UTC (rev 117)
+++ trunk/test/01-testfile/dumpscript/12-TrailingParentDir.test	2007-01-23 10:08:23 UTC (rev 118)
@@ -15,8 +15,6 @@
 	config="yes!"
 EOL
 
-echo "$di/tmtest.conf" >> /tmp/tt
-
 MKFILE cc2 "$di/tmtest.conf" <<'EOL'
 	config="yes!"
 EOL

Modified: trunk/test/02-running/50-OpenFDsTest.test
===================================================================
--- trunk/test/02-running/50-OpenFDsTest.test	2007-01-23 09:48:03 UTC (rev 117)
+++ trunk/test/02-running/50-OpenFDsTest.test	2007-01-23 10:08:23 UTC (rev 118)
@@ -1,20 +1,25 @@
 # Ensure we don't leak fds to the running test.
 # Ideally, there would be NO open FDs.  This will be the case for tmtest 0.98.
 
+# This used to output the following text but platforms just differ too
+# much exactly what fds they leave open.  So, just so long as there's 4
+# fds open, we're as happy as we can be.
+#    open: 4
+#    open: 6
+#    open: 7
+#    open: 10
+
 $tmtest -o -q - <<-'EOL' | INDENT
 	for i in `seq 3 255`; do
 		exec 2>/dev/null
 		echo -n >&$i && echo open: $i
-	done
+	done | wc -l
 EOL
 
 STDOUT:
     for i in `seq 3 255`; do
     exec 2>/dev/null
     echo -n >&$i && echo open: $i
-    done
+    done | wc -l
     STDOUT:
-    open: 4
-    open: 6
-    open: 7
-    open: 10
+    4

Modified: trunk/test/03-results/32-BinaryLarge.test
===================================================================
--- trunk/test/03-results/32-BinaryLarge.test	2007-01-23 09:48:03 UTC (rev 117)
+++ trunk/test/03-results/32-BinaryLarge.test	2007-01-23 10:08:23 UTC (rev 118)
@@ -1,7 +1,7 @@
 # Ensures that a testfile can contain some arbitrary binary
 # data and the test will still work.  This just tests control chars.
 
-# We need to manuall create the testfile because Bash will screw 
+# We need to manually create the testfile because Bash will screw 
 # things up if we use a heredoc.
 
 MKFILE tt <<-'EOL'





From tmtest-commits at lists.berlios.de  Tue Jan 23 11:27:50 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Tue, 23 Jan 2007 10:27:50 -0000
Subject: [Tmtest-commits] [119] trunk: Further eradicate MODIFY from the
	source tree and the
Message-ID: <200701231026.l0NAQPYL016869@sheep.berlios.de>

Revision: 119
Author:   bronson
Date:     2007-01-23 11:26:24 +0100 (Tue, 23 Jan 2007)

Log Message:
-----------
Further eradicate MODIFY from the source tree and the documentation.

Modified Paths:
--------------
    trunk/TODO
    trunk/test/02-running/50-OpenFDsTest.test
    trunk/tfscan.c
    trunk/tfscan.h
    trunk/tmtest.pod
Modified: trunk/TODO
===================================================================
--- trunk/TODO	2007-01-23 10:08:23 UTC (rev 118)
+++ trunk/TODO	2007-01-23 10:26:24 UTC (rev 119)
@@ -134,9 +134,6 @@
 - Unify the line modifier in compare.c and test.c.  It's hackish now.
 - Wow, the pcrs error messages truly suck.  Is there any way to improve them?
   "(pcrs:) Syntax error while parsing command (-11)."
-- add a CLEAR_EACHLINE or CLEAR_MODIFY option to to turn off all substitutions.
-  It would be easy to do.  I'm just not convinced that anyone would find it
-  useful.
 
 ????:
 - There should be a way to repeatedly run a single test with only tiny

Modified: trunk/test/02-running/50-OpenFDsTest.test
===================================================================
--- trunk/test/02-running/50-OpenFDsTest.test	2007-01-23 10:08:23 UTC (rev 118)
+++ trunk/test/02-running/50-OpenFDsTest.test	2007-01-23 10:26:24 UTC (rev 119)
@@ -22,4 +22,4 @@
     echo -n >&$i && echo open: $i
     done | wc -l
     STDOUT:
-    4
+    3

Modified: trunk/tfscan.c
===================================================================
--- trunk/tfscan.c	2007-01-23 10:08:23 UTC (rev 118)
+++ trunk/tfscan.c	2007-01-23 10:26:24 UTC (rev 119)
@@ -21,16 +21,10 @@
 // 		And a keyword without a NL is still the keyword.
 // 		But it must always start at the beginning of a new line.
 // exit clauses with invalid numbers
-// What happens with a MODIFY larger than BUFSIZ.
 // DOS/Mac/Unix line endings.
 // 		What happes when platform doesn't match the testfile?
 // 	Get rid of rewrite_command_section
 
-// NOTE: because we linebuffer the MODIFY clause, a single MODIFY
-// may not be larger than the BUFSIZ on your system (usually 8192 bytes).
-// Technically this is true of RESULT as well but when are you ever going
-// to run into an 8K result code?
-
 #include "tfscan.h"
 
 
@@ -56,7 +50,6 @@
 "STDOUT" WS* ":" ANYN* "\n"  { START(exSTDOUT); return exNEW|exSTDOUT; }
 "STDERR" WS* ":" ANYN* "\n"  { START(exSTDERR); return exNEW|exSTDERR; }
 "RESULT" WS* ":" ANYN* "\n"  { START(exRESULT); return exNEW|exRESULT; }
-"MODIFY" WS* ":" ANYN* "\n"  { START(exMODIFY); return exNEW|exMODIFY; }
 
 ANYN* "\n"                  { return (int)ss->scanref; }
 
@@ -217,14 +210,6 @@
 					return scan_to_end_of_keyword(ss, exRESULT);
 				}
 				break;
-			case 'M':
-				if(YYCURSOR[1]=='O' && YYCURSOR[2]=='D' &&
-					YYCURSOR[3]=='I' && YYCURSOR[4]=='F' && YYCURSOR[5]=='Y')
-				{
-                    YYCURSOR += 6;
-					return scan_to_end_of_keyword(ss, exMODIFY);
-				}
-				break;
 			default:
 				break;
 		}

Modified: trunk/tfscan.h
===================================================================
--- trunk/tfscan.h	2007-01-23 10:08:23 UTC (rev 118)
+++ trunk/tfscan.h	2007-01-23 10:26:24 UTC (rev 119)
@@ -24,7 +24,6 @@
     exSTDOUT = 64,			///< marks a line in the stdout section.
     exSTDERR,				///< marks a line in the stderr section.
     exRESULT,				///< marks a line in the result (exit value) section.
-    exMODIFY,				///< marks a line in the modify (per-line substitution) section.
     exRESULT_TOKEN_END,		///< never returned.  this token is always one higher than the highest-numbered section token.
 
     exNEW = 0x100,			///< flag added to the section token that specifies that this is the start of a new section.

Modified: trunk/tmtest.pod
===================================================================
--- trunk/tmtest.pod	2007-01-23 10:08:23 UTC (rev 118)
+++ trunk/tmtest.pod	2007-01-23 10:26:24 UTC (rev 119)
@@ -59,21 +59,6 @@
 
 =over 8
 
-=item MODIFY
-
-Specifies a perl-compatible substitution command to be executed
-on each line of the program's output.  For instance,
-the following modify clause will replace DATE='2005-10-02' with
-DATE='DATE DATE'.
-
-	MODIFY: s/DATE='.*'/DATE='DATE DATE'/
-
-Due to a pcrs limitation, you may only specify one substitution
-command per MODIFY clause.  ("MODIFY: s/1/one/ s/2/two/" will not
-work.
-
-=back
-
 =head1 CONFIGURATION
 
 tmtest reads its configuration first from F</etc/tmtest.conf> and
@@ -146,21 +131,19 @@
 =head1 FAQ
 
 How do I indent my stdout section?  My test prints the word "STDOUT" at
-the beginning of the line, causing tmtest to choke on the multiple STDOUT
+the beginning of a line, causing tmtest to complain about multiple STDOUT
 sections.
 
-A: Yes, tmtest is pedantic about finding sections in a file.  This is
-because it can be easy to get confused about what line applies where.
-Easiest just to ensure that there is a single section.
+Answer: run your command's output through indent.
 
-But what to do if your test's output would contain a section break?
-Simply indent every line of your test's output by some number of spaces:
+	mycmd | INDENT
 
-	MODIFY : s/^(.)/   $1/
+If you have multiple commands, you can use parentheses to collect the
+output:
 
-The complex expression is because some versions of pcrs mistakenly
-add two spaces after tine final newline.  This ensures that the line
-actually contains a character before indenting it.
+	(	cmd1
+		cmd2
+	) 2>&1 | INDENT
 
 =head1 HISTORY
 





From tmtest-commits at lists.berlios.de  Wed Jan 24 03:52:07 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Wed, 24 Jan 2007 02:52:07 -0000
Subject: [Tmtest-commits] [120] trunk/test/03-results: Remove the RESULT
	section from tmtest
Message-ID: <200701240250.l0O2ofif026118@sheep.berlios.de>

Revision: 120
Author:   bronson
Date:     2007-01-24 03:50:38 +0100 (Wed, 24 Jan 2007)

Log Message:
-----------
Remove the RESULT section from tmtest entirely.
Use $? from within the testfile instead.

Modified Paths:
--------------
    trunk/test/00-cmdline/10-CfgDir.test
    trunk/test/00-cmdline/11-CfgMissing.test
    trunk/test/00-cmdline/12-CfgEmpty.test
    trunk/test/01-testfile/05-MultipleStdout.test
    trunk/test/01-testfile/06-MultipleStderr.test
    trunk/test/01-testfile/08-GarbageInStdout.test
    trunk/test/01-testfile/09-GarbageInStderr.test
    trunk/test/01-testfile/dumpscript/10-Stdout.test
    trunk/test/01-testfile/dumpscript/11-AbsoluteDir.test
    trunk/test/01-testfile/dumpscript/12-TrailingParentDir.test
    trunk/test/01-testfile/dumpscript/13-BareParentDir.test
    trunk/test/01-testfile/dumpscript/14-DeepDir.test
    trunk/test/01-testfile/dumpscript/15-PathOps.test
    trunk/test/01-testfile/dumpscript/16-AbsoluteDir.test
    trunk/test/02-running/18-TestError.test
    trunk/test/03-results/02-Nstderr.test
    trunk/test/03-results/02-Nstdout.test
    trunk/test/03-results/03-Nstderr.test
    trunk/test/03-results/03-Nstdout.test
    trunk/test/03-results/04-Nstderr.test
    trunk/test/03-results/04-Nstdout.test
    trunk/test/03-results/10-NoNLErrFail.test
    trunk/test/03-results/10-NoNLFFail.test
    trunk/test/03-results/10-NoNLFail.test
    trunk/test/03-results/13-MultiNL.test
    trunk/test/03-results/13-MultiNL2.test
    trunk/test/03-results/13-MultiNL2Err.test
    trunk/test/03-results/13-MultiNLErr.test
    trunk/test/03-results/14-NoNLWarn.test
    trunk/test/03-results/31-BinaryFail.test

Removed Paths:
-------------
    trunk/test/01-testfile/01-ExitGarbageWarn.test
Modified: trunk/test/00-cmdline/10-CfgDir.test
===================================================================
--- trunk/test/00-cmdline/10-CfgDir.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/00-cmdline/10-CfgDir.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -1,7 +1,9 @@
 $tmtest --config=/tmp - <<-EOL
 	echo Test!
 EOL
+echo RESULT: $?
 
-RESULT: 2
 STDERR:
 Could not open /tmp: not a file!
+STDOUT:
+RESULT: 2

Modified: trunk/test/00-cmdline/11-CfgMissing.test
===================================================================
--- trunk/test/00-cmdline/11-CfgMissing.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/00-cmdline/11-CfgMissing.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -15,6 +15,5 @@
 	Howdy
 EOL
 
-RESULT: 0
 STDOUT:
 Could not locate ...zzyzx: No such file or directory

Modified: trunk/test/00-cmdline/12-CfgEmpty.test
===================================================================
--- trunk/test/00-cmdline/12-CfgEmpty.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/00-cmdline/12-CfgEmpty.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -3,7 +3,9 @@
 	STDOUT:
 	Howdy
 EOL
+echo RESULT: $?
 
-RESULT: 1
 STDERR:
 You must specify a directory for --config.
+STDOUT:
+RESULT: 1

Deleted: trunk/test/01-testfile/01-ExitGarbageWarn.test
===================================================================
--- trunk/test/01-testfile/01-ExitGarbageWarn.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/01-ExitGarbageWarn.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -1,17 +0,0 @@
-# Ensure we emit a warning if the user includes garbage in his
-# exit value section.
-
-$tmtest -q - <<-EOL
-	echo Howdy
-	RESULT: 0
-	junque
-	STDOUT:
-	Howdy
-EOL
-
-STDERR:
-(STDIN) line 3 Error: RESULT clause contains garbage.
-STDOUT:
-ok   (STDIN) 
-
-1 test run, 1 success, 0 failures.

Modified: trunk/test/01-testfile/05-MultipleStdout.test
===================================================================
--- trunk/test/01-testfile/05-MultipleStdout.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/05-MultipleStdout.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -15,6 +15,6 @@
 STDERR:
 (STDIN) line 7 Error: duplicate STDOUT section.  Ignored.
 STDOUT:
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.

Modified: trunk/test/01-testfile/06-MultipleStderr.test
===================================================================
--- trunk/test/01-testfile/06-MultipleStderr.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/06-MultipleStderr.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -12,7 +12,6 @@
 
 STDERR:
 (STDIN) line 5 Error: duplicate STDERR section.  Ignored.
-RESULT: 0
 STDOUT:
 ok   (STDIN) 
 

Modified: trunk/test/01-testfile/08-GarbageInStdout.test
===================================================================
--- trunk/test/01-testfile/08-GarbageInStdout.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/08-GarbageInStdout.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -11,6 +11,6 @@
 STDERR
 (STDIN) line 2: unknown arguments "crapola"
 STDOUT
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.

Modified: trunk/test/01-testfile/09-GarbageInStderr.test
===================================================================
--- trunk/test/01-testfile/09-GarbageInStderr.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/09-GarbageInStderr.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -13,6 +13,6 @@
 STDERR:
 (STDIN) line 4: unknown arguments "crapola"
 STDOUT:
-FAIL (STDIN)                   OE.  stdout and stderr differed
+FAIL (STDIN)                   OE  stdout and stderr differed
 
 1 test run, 0 successes, 1 failure.

Modified: trunk/test/01-testfile/dumpscript/10-Stdout.test
===================================================================
--- trunk/test/01-testfile/dumpscript/10-Stdout.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/dumpscript/10-Stdout.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -36,8 +36,6 @@
 	STDOUT: () { exit 0; }
 	STDERR () { exit 0; }
 	STDERR: () { exit 0; }
-	RESULT () { exit 0; }
-	RESULT: () { exit 0; }
 	
 	echo 'RUNNING: (STDIN)' >&FD
 	MYDIR='/tmp'

Modified: trunk/test/01-testfile/dumpscript/11-AbsoluteDir.test
===================================================================
--- trunk/test/01-testfile/dumpscript/11-AbsoluteDir.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/dumpscript/11-AbsoluteDir.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -51,8 +51,6 @@
 	STDOUT: () { exit 0; }
 	STDERR () { exit 0; }
 	STDERR: () { exit 0; }
-	RESULT () { exit 0; }
-	RESULT: () { exit 0; }
 	
 	echo 'RUNNING: /tmp/DIR/tt.test' >&FD
 	MYDIR='/tmp/DIR'

Modified: trunk/test/01-testfile/dumpscript/12-TrailingParentDir.test
===================================================================
--- trunk/test/01-testfile/dumpscript/12-TrailingParentDir.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/dumpscript/12-TrailingParentDir.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -65,8 +65,6 @@
 	STDOUT: () { exit 0; }
 	STDERR () { exit 0; }
 	STDERR: () { exit 0; }
-	RESULT () { exit 0; }
-	RESULT: () { exit 0; }
 	
 	echo 'RUNNING: /tmp/DIR/di/tt.test' >&FD
 	MYDIR='/tmp/DIR/di'
@@ -75,5 +73,4 @@
 	. '/tmp/DIR/di/tt.test'
 	
 	echo DONE >&FD
-RESULT: 0
 STDERR:

Modified: trunk/test/01-testfile/dumpscript/13-BareParentDir.test
===================================================================
--- trunk/test/01-testfile/dumpscript/13-BareParentDir.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/dumpscript/13-BareParentDir.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -66,8 +66,6 @@
 	STDOUT: () { exit 0; }
 	STDERR () { exit 0; }
 	STDERR: () { exit 0; }
-	RESULT () { exit 0; }
-	RESULT: () { exit 0; }
 	
 	echo 'RUNNING: /tmp/DIR/di/tt.test' >&FD
 	MYDIR='/tmp/DIR/di'
@@ -76,5 +74,4 @@
 	. '/tmp/DIR/di/tt.test'
 	
 	echo DONE >&FD
-RESULT: 0
 STDERR:

Modified: trunk/test/01-testfile/dumpscript/14-DeepDir.test
===================================================================
--- trunk/test/01-testfile/dumpscript/14-DeepDir.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/dumpscript/14-DeepDir.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -68,8 +68,6 @@
 	STDOUT: () { exit 0; }
 	STDERR () { exit 0; }
 	STDERR: () { exit 0; }
-	RESULT () { exit 0; }
-	RESULT: () { exit 0; }
 	
 	echo 'RUNNING: /tmp/DIR/di/tt.test' >&FD
 	MYDIR='/tmp/DIR/di'
@@ -78,5 +76,4 @@
 	. '/tmp/DIR/di/tt.test'
 	
 	echo DONE >&FD
-RESULT: 0
 STDERR:

Modified: trunk/test/01-testfile/dumpscript/15-PathOps.test
===================================================================
--- trunk/test/01-testfile/dumpscript/15-PathOps.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/dumpscript/15-PathOps.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -46,8 +46,6 @@
 	STDOUT: () { exit 0; }
 	STDERR () { exit 0; }
 	STDERR: () { exit 0; }
-	RESULT () { exit 0; }
-	RESULT: () { exit 0; }
 	
 	echo 'RUNNING: /tmp/DIR/tt.test' >&FD
 	MYDIR='/tmp/DIR'

Modified: trunk/test/01-testfile/dumpscript/16-AbsoluteDir.test
===================================================================
--- trunk/test/01-testfile/dumpscript/16-AbsoluteDir.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/01-testfile/dumpscript/16-AbsoluteDir.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -67,8 +67,6 @@
 	STDOUT: () { exit 0; }
 	STDERR () { exit 0; }
 	STDERR: () { exit 0; }
-	RESULT () { exit 0; }
-	RESULT: () { exit 0; }
 	
 	echo 'RUNNING: DIR/di/tt.test' >&FD
 	MYDIR='DIR/di'
@@ -77,5 +75,4 @@
 	. 'DIR/di/tt.test'
 	
 	echo DONE >&FD
-RESULT: 0
 STDERR:

Modified: trunk/test/02-running/18-TestError.test
===================================================================
--- trunk/test/02-running/18-TestError.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/02-running/18-TestError.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -1,5 +1,8 @@
 # Sees what happens when we're in pedantic mode and there's an error
 # in the test file.
+#
+# Now that we no longer have a RESULT section, I'm not sure this test
+# means much...   leaving it in anyway.
 
 $tmtest $args -o - <<-EOL | sed -re "s/: line [0-9]+:/: line BLAH:/" | INDENT "  "
 	set -e
@@ -15,6 +18,5 @@
   iusedtobullseyewompratsinmypants
   echo it ran
   STDOUT:
-  RESULT: 127
   STDERR:
   /bin/bash: line BLAH: iusedtobullseyewompratsinmypants: command not found

Modified: trunk/test/03-results/02-Nstderr.test
===================================================================
--- trunk/test/03-results/02-Nstderr.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/02-Nstderr.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -10,7 +10,6 @@
 	howdy
 EOL
 STDOUT:
-FAIL (STDIN)                   .E.  stderr differed
+FAIL (STDIN)                   .E  stderr differed
 
 1 test run, 0 successes, 1 failure.
-RESULT: 0

Modified: trunk/test/03-results/02-Nstdout.test
===================================================================
--- trunk/test/03-results/02-Nstdout.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/02-Nstdout.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -10,7 +10,6 @@
 	howdy
 EOL
 STDOUT: 
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.
-RESULT: 0

Modified: trunk/test/03-results/03-Nstderr.test
===================================================================
--- trunk/test/03-results/03-Nstderr.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/03-Nstderr.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -10,7 +10,6 @@
 	Howdy 
 EOL
 STDOUT:
-FAIL (STDIN)                   .E.  stderr differed
+FAIL (STDIN)                   .E  stderr differed
 
 1 test run, 0 successes, 1 failure.
-RESULT: 0

Modified: trunk/test/03-results/03-Nstdout.test
===================================================================
--- trunk/test/03-results/03-Nstdout.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/03-Nstdout.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -11,7 +11,6 @@
 	Howdy 
 EOL
 STDOUT: 
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.
-RESULT: 0

Modified: trunk/test/03-results/04-Nstderr.test
===================================================================
--- trunk/test/03-results/04-Nstderr.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/04-Nstderr.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -13,7 +13,6 @@
 
 
 STDOUT:
-FAIL (STDIN)                   .E.  stderr differed
+FAIL (STDIN)                   .E  stderr differed
 
 1 test run, 0 successes, 1 failure.
-RESULT: 0

Modified: trunk/test/03-results/04-Nstdout.test
===================================================================
--- trunk/test/03-results/04-Nstdout.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/04-Nstdout.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -12,7 +12,6 @@
 
 EOL
 STDOUT: 
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.
-RESULT: 0

Modified: trunk/test/03-results/10-NoNLErrFail.test
===================================================================
--- trunk/test/03-results/10-NoNLErrFail.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/10-NoNLErrFail.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -12,6 +12,6 @@
 	Howdy
 EOL
 STDOUT : 
-FAIL (STDIN)                   .E.  stderr differed
+FAIL (STDIN)                   .E  stderr differed
 
 1 test run, 0 successes, 1 failure.

Modified: trunk/test/03-results/10-NoNLFFail.test
===================================================================
--- trunk/test/03-results/10-NoNLFFail.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/10-NoNLFFail.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -9,7 +9,7 @@
 
 EOL
 STDOUT : 
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.
 STDERR:

Modified: trunk/test/03-results/10-NoNLFail.test
===================================================================
--- trunk/test/03-results/10-NoNLFail.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/10-NoNLFail.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -16,6 +16,6 @@
 	Howdy
 EOL
 STDOUT : 
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.

Modified: trunk/test/03-results/13-MultiNL.test
===================================================================
--- trunk/test/03-results/13-MultiNL.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/13-MultiNL.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -9,7 +9,7 @@
 
 EOL
 STDOUT: 
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.
 STDERR:

Modified: trunk/test/03-results/13-MultiNL2.test
===================================================================
--- trunk/test/03-results/13-MultiNL2.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/13-MultiNL2.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -11,6 +11,6 @@
 EOL
 
 STDOUT: 
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.

Modified: trunk/test/03-results/13-MultiNL2Err.test
===================================================================
--- trunk/test/03-results/13-MultiNL2Err.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/13-MultiNL2Err.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -11,6 +11,6 @@
 EOL
 
 STDOUT:
-FAIL (STDIN)                   .E.  stderr differed
+FAIL (STDIN)                   .E  stderr differed
 
 1 test run, 0 successes, 1 failure.

Modified: trunk/test/03-results/13-MultiNLErr.test
===================================================================
--- trunk/test/03-results/13-MultiNLErr.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/13-MultiNLErr.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -9,7 +9,7 @@
 
 EOL
 STDOUT: 
-FAIL (STDIN)                   .E.  stderr differed
+FAIL (STDIN)                   .E  stderr differed
 
 1 test run, 0 successes, 1 failure.
 STDERR:

Modified: trunk/test/03-results/14-NoNLWarn.test
===================================================================
--- trunk/test/03-results/14-NoNLWarn.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/14-NoNLWarn.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -14,7 +14,7 @@
 	Howdy
 EOL
 STDOUT : 
-FAIL (STDIN)                   O..  stdout differed
+FAIL (STDIN)                   O.  stdout differed
 
 1 test run, 0 successes, 1 failure.
 STDERR:

Modified: trunk/test/03-results/31-BinaryFail.test
===================================================================
--- trunk/test/03-results/31-BinaryFail.test	2007-01-23 10:26:24 UTC (rev 119)
+++ trunk/test/03-results/31-BinaryFail.test	2007-01-24 02:50:38 UTC (rev 120)
@@ -17,6 +17,6 @@
 $tmtest --ignore-extension -q "$tt" | REPLACE "$tt" /tmp/FILE
 
 STDOUT:
-FAIL /tmp/FILE        O..  stdout differed
+FAIL /tmp/FILE        O.  stdout differed
 
 1 test run, 0 successes, 1 failure.





From tmtest-commits at lists.berlios.de  Wed Jan 24 03:54:50 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Wed, 24 Jan 2007 02:54:50 -0000
Subject: [Tmtest-commits] [121] trunk: Remove RESULT section from testfiles.
Message-ID: <200701240253.l0O2rTgC026256@sheep.berlios.de>

Revision: 121
Author:   bronson
Date:     2007-01-24 03:53:28 +0100 (Wed, 24 Jan 2007)

Log Message:
-----------
Remove RESULT section from testfiles.

Modified Paths:
--------------
    trunk/CHANGES
    trunk/TODO
    trunk/main.c
    trunk/template.sh
    trunk/test.c
    trunk/test.h
    trunk/tfscan.c
    trunk/tfscan.h
    trunk/tmtest.pod
Modified: trunk/CHANGES
===================================================================
--- trunk/CHANGES	2007-01-24 02:50:38 UTC (rev 120)
+++ trunk/CHANGES	2007-01-24 02:53:28 UTC (rev 121)
@@ -1,3 +1,4 @@
+- Got rid of RESULT and all its code.  echo $? from within your test instead.
 - Got rid of Cutest since it was very non-cute.  Wrote zutest instead.
 - Changed --all-files to be the much more understandable --ignore-extension.
 - If an output section is marked -n but ends with multiple newlines, we print

Modified: trunk/TODO
===================================================================
--- trunk/TODO	2007-01-24 02:50:38 UTC (rev 120)
+++ trunk/TODO	2007-01-24 02:53:28 UTC (rev 121)
@@ -1,7 +1,12 @@
 0.96:
+- Add a cmdline option that prints only tests that fail, and prints their
+  full path.
+- Is it possible to separate STDOUT and STDERR?  Maybe stderr comes first
+  in the testfile with each line prefixed by :.  Then STDOUT.  No need
+  for this delimiter craziness.
+  YES, stderr ALWAYS comes first in the testfile, followed by stdout.
 - MKFILE should just create the file that the user names.
 - MKTMPFILE should create a filename and store that in the user's variable.
-
 - Write unit tests for path normalization.
 - Make tmtest only execute config files owned by either the user or root.
   Print a big fat warning when the config file is skipped.  This prevents

Modified: trunk/main.c
===================================================================
--- trunk/main.c	2007-01-24 02:50:38 UTC (rev 120)
+++ trunk/main.c	2007-01-24 02:53:28 UTC (rev 121)
@@ -550,7 +550,7 @@
         i = wait_for_child(child, "test");
         test.exitsignal = (WIFSIGNALED(i) ? WTERMSIG(i) : 0);
         test.exitcored = (WIFSIGNALED(i) ? WCOREDUMP(i) : 0);
-        test.exitno = (WIFEXITED(i) ? WEXITSTATUS(i) : 256);
+        // test.exitno = (WIFEXITED(i) ? WEXITSTATUS(i) : 256);
 
         // read the status file to determine what happened
         // and store the information in the test struct.

Modified: trunk/template.sh
===================================================================
--- trunk/template.sh	2007-01-24 02:50:38 UTC (rev 120)
+++ trunk/template.sh	2007-01-24 02:53:28 UTC (rev 121)
@@ -19,8 +19,6 @@
 STDOUT: () { exit 0; }
 STDERR () { exit 0; }
 STDERR: () { exit 0; }
-RESULT () { exit 0; }
-RESULT: () { exit 0; }
 
 echo 'RUNNING: %(TESTFILE)' >&%(STATUSFD)
 MYDIR='%(TESTDIR)'

Modified: trunk/test.c
===================================================================
--- trunk/test.c	2007-01-24 02:50:38 UTC (rev 120)
+++ trunk/test.c	2007-01-24 02:53:28 UTC (rev 121)
@@ -354,53 +354,6 @@
 }
 
 
-/** Returns true if the given buffer contains non-whitespace characters,
- * false if the buffer consists entirely of whitespace. */
-
-static int contains_nws(const char *cp, int len)
-{
-	const char *ce = cp + len;
-
-	while(cp < ce) {
-		if(!isspace(*cp)) {
-			return 1;
-		}
-	}
-
-	return 0;
-}
-
-
-/** Scans the given buffer for the exit value.
- *
- * Ignores everything except for the first digit and any digits that
- * follow it.
- *
- * If digits are found, then it updates the test structure with
- * whether the exit values match or not.
- * If no digits are found, then this routine does nothing.
- */
-
-void parse_exit_clause(struct test *test, const char *cp, int len)
-{
-	const char *ce = cp + len;
-	unsigned int num = 0;
-
-	// skip to the first digit in the buffer
-	while(!isdigit(*cp) && cp < ce) cp++;
-	if(cp >= ce) return;
-
-	// scan the number
-	while(isdigit(*cp)) {
-		num = 10*num + (*cp - '0');
-		cp++;
-	}
-
-	test->expected_exitno = num;
-	test->exitno_match = (test->exitno == num ? match_yes : match_no);
-}
-
-
 /** Increments cp past the section name.
  *
  * Will not increment cp by more than len bytes.
@@ -675,9 +628,6 @@
                     cmpscan_state = 0;
                 }
                 break;
-            case exRESULT:
-				parse_exit_clause(test, datap, len);
-                break;
         }
     } else {
         // we're continuing an already started section.
@@ -691,14 +641,6 @@
             case exSTDERR:
 				compare_continue(cmpscan, datap, len);
                 break;
-            case exRESULT:
-				if(contains_nws(datap, len)) {
-					fprintf(stderr, "%s line %d Error: RESULT clause "
-                            "contains garbage.\n",
-							get_testfile_name(test), test->testfile.line);
-                    // Harmless to continue.  The testfile needs to be fixed.
-				}
-                break;
             case exCOMMAND:
                 break;
         }
@@ -771,7 +713,7 @@
 {
     scanstate scanner;
     char scanbuf[BUFSIZ];
-	int stdo, stde, exno;	// true if there are differences.
+	int stdo, stde;	// true if there are differences.
 	
 	if(was_aborted(test->status)) {
 		print_reason(test, "ABRT", "by");
@@ -791,7 +733,6 @@
         return;
     }
 
-    test->exitno_match = match_unknown;
     test->stdout_match = match_unknown;
     test->stderr_match = match_unknown;
 
@@ -801,11 +742,6 @@
     assert(test->stdout_match != match_inprogress);
     assert(test->stderr_match != match_inprogress);
 
-    // convert any unknowns into a solid yes/no
-    if(test->exitno_match == match_unknown) {
-		test->expected_exitno = 0;
-        test->exitno_match = (test->exitno == 0 ? match_yes : match_no);
-    }
     if(test->stdout_match == match_unknown) {
         test->stdout_match = (fd_has_data(test->outfd) ? match_no : match_yes);
     }
@@ -815,9 +751,8 @@
 
     stdo = (test->stdout_match != match_yes);
     stde = (test->stderr_match != match_yes);
-    exno = (test->exitno_match != match_yes);
 
-    if(!stdo && !stde && !exno) {
+    if(!stdo && !stde && !test->exitsignal) {
         test_successes++;
         printf("ok   %s \n", convert_testfile_name(dispname));
     } else {
@@ -827,19 +762,15 @@
             printf("terminated by signal %d%s", test->exitsignal,
                     (test->exitcored ? " with core" : ""));
         } else {
-            printf("%c%c%c  ",
+            printf("%c%c  ",
                     (stdo ? 'O' : '.'),
-                    (stde ? 'E' : '.'),
-                    (exno ? 'X' : '.'));
+                    (stde ? 'E' : '.'));
             if(stdo || stde) {
                 if(stdo) printf("stdout ");
                 if(stdo && stde) printf("and ");
                 if(stde) printf("stderr ");
                 printf("differed");
             }
-            if((stdo || stde) && exno) printf(", ");
-            if(exno) printf("result was %d not %d",
-                    test->exitno, test->expected_exitno);
         }
 		printf("\n");
     }
@@ -848,16 +779,6 @@
 }
 
 
-static void write_exit_no(int fd, int exitno)
-{
-    char buf[512];
-    int cnt;
-
-    cnt = snprintf(buf, sizeof(buf), "RESULT: %d\n", exitno);
-    write(fd, buf, cnt);
-}
-
-
 /**
  * Reads all the data from infd and writes it onto outfd.
  *
@@ -970,15 +891,6 @@
             // ignore all data in the expected stderr
             break;
 
-        case exRESULT|exNEW:
-            test->exitno_match = match_yes;
-            write_exit_no(test->rewritefd, test->exitno);
-            break;
-        case exRESULT:
-            // allow random garbage in result section to pass
-            write(test->rewritefd, datap, len);
-            break;
-
         default:
             write(test->rewritefd, datap, len);
     }
@@ -1033,7 +945,6 @@
     // that we need to dump them in the same order as they occur
     // in the testfile otherwise the diff will be all screwed up.
 
-    test->exitno_match = match_unknown;
     test->stdout_match = match_unknown;
     test->stderr_match = match_unknown;
 
@@ -1041,10 +952,6 @@
 
     // if any sections haven't been output, but they differ from
     // the default, then they need to be output here at the end.
-
-    if(test->exitno_match == match_unknown && test->exitno != 0) {
-        write_exit_no(test->rewritefd, test->exitno);
-    }
     if(test->stderr_match == match_unknown && fd_has_data(test->errfd)) {
 		write_strconst(test->rewritefd, "STDERR:\n");
         write_file(test->rewritefd, test->errfd, NULL);

Modified: trunk/test.h
===================================================================
--- trunk/test.h	2007-01-24 02:50:38 UTC (rev 120)
+++ trunk/test.h	2007-01-24 02:53:28 UTC (rev 121)
@@ -60,10 +60,10 @@
     int outfd;				///< the file that receives the test's stdout.
     int errfd;				///< the file that receives the test's stderr.
     int statusfd;			///< receives the runtime test status messages.
-    int exitno;				///< the test's actual exit value (WEXITSTATUS).
     int exitsignal;         ///< the value returned for the test by waitpid(2)
     int exitcored;          ///< if exitsignal is true, true if child core dumped.
 
+
 	char *diffname;			///< if we're diffing against stdin, this contains the name of the required tempfile.
 	int diff_fd;			///< if diffname is set, then this is the fd of the tempfile we're using to store stdin.
 
@@ -74,9 +74,6 @@
 	char *last_file_processed; ///< if it could be discovered, this contains the name of the last file to be started.  must be freed.
 	int aborted;			///< true if the test was aborted (and therefore no further tests should be run).
 
-	int expected_exitno;	///< the test's expected exit value.  this is only valid when stderr_match != match_unknown.
-
-    enum matchval exitno_match;	///< tells whether the expected and actual exit values match.
     enum matchval stdout_match;	///< tells whether the expected and actual stdout matches.
     enum matchval stderr_match;	///< tells whether the expected and actual stderr matches.
 };

Modified: trunk/tfscan.c
===================================================================
--- trunk/tfscan.c	2007-01-24 02:50:38 UTC (rev 120)
+++ trunk/tfscan.c	2007-01-24 02:53:28 UTC (rev 121)
@@ -49,7 +49,6 @@
 
 "STDOUT" WS* ":" ANYN* "\n"  { START(exSTDOUT); return exNEW|exSTDOUT; }
 "STDERR" WS* ":" ANYN* "\n"  { START(exSTDERR); return exNEW|exSTDERR; }
-"RESULT" WS* ":" ANYN* "\n"  { START(exRESULT); return exNEW|exRESULT; }
 
 ANYN* "\n"                  { return (int)ss->scanref; }
 
@@ -202,14 +201,6 @@
 				}
 				// else it wasn't a token so we can just keep scanning.
 				break;
-			case 'R':
-				if(YYCURSOR[1]=='E' && YYCURSOR[2]=='S' &&
-					YYCURSOR[3]=='U' && YYCURSOR[4]=='L' && YYCURSOR[5]=='T')
-				{
-                    YYCURSOR += 6;
-					return scan_to_end_of_keyword(ss, exRESULT);
-				}
-				break;
 			default:
 				break;
 		}

Modified: trunk/tfscan.h
===================================================================
--- trunk/tfscan.h	2007-01-24 02:50:38 UTC (rev 120)
+++ trunk/tfscan.h	2007-01-24 02:53:28 UTC (rev 121)
@@ -23,7 +23,6 @@
     // result sections are numbered from 64 through 127.
     exSTDOUT = 64,			///< marks a line in the stdout section.
     exSTDERR,				///< marks a line in the stderr section.
-    exRESULT,				///< marks a line in the result (exit value) section.
     exRESULT_TOKEN_END,		///< never returned.  this token is always one higher than the highest-numbered section token.
 
     exNEW = 0x100,			///< flag added to the section token that specifies that this is the start of a new section.

Modified: trunk/tmtest.pod
===================================================================
--- trunk/tmtest.pod	2007-01-24 02:50:38 UTC (rev 120)
+++ trunk/tmtest.pod	2007-01-24 02:53:28 UTC (rev 121)
@@ -134,6 +134,8 @@
 the beginning of a line, causing tmtest to complain about multiple STDOUT
 sections.
 
+=over 4
+
 Answer: run your command's output through indent.
 
 	mycmd | INDENT
@@ -145,6 +147,27 @@
 		cmd2
 	) 2>&1 | INDENT
 
+=back
+
+How do I retrieve my test program's exit result?
+
+=over 4
+
+Answer: use the shell variable $?:
+
+    myprogram
+	echo RESULT: $?
+
+tmtest used to provide a RESULT section that automatically checked a
+command's result value.  The problem is, most testfiles contain
+multiple commands, with cleanup code, etc.  To ensure that tmtest
+reads the correct value of $? would require drastic restrictions
+on the things a testfile may do.  We decided that it was much better
+to force the user to insert $? in the proper spot rather than add
+a whole bunch of arbitrary restrictions.
+
+=back
+
 =head1 HISTORY
 
 The original tmtest script was written in June of 2000 when I got frustrated





From tmtest-commits at lists.berlios.de  Wed Jan 24 04:05:22 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Wed, 24 Jan 2007 03:05:22 -0000
Subject: [Tmtest-commits] [122] trunk: Add a testfile to ensure that all
	unit tests pass.
Message-ID: <200701240303.l0O33qws027009@sheep.berlios.de>

Revision: 122
Author:   bronson
Date:     2007-01-24 04:03:51 +0100 (Wed, 24 Jan 2007)

Log Message:
-----------
Add a testfile to ensure that all unit tests pass.

Modified Paths:
--------------
    trunk/main.c
    trunk/zutest.c

Added Paths:
-----------
    trunk/test/02-running/03-Unit-Tests.test
Modified: trunk/main.c
===================================================================
--- trunk/main.c	2007-01-24 02:53:28 UTC (rev 121)
+++ trunk/main.c	2007-01-24 03:03:51 UTC (rev 122)
@@ -1062,6 +1062,10 @@
 				quiet++;
 				break;
 
+			case 'U':
+				run_unit_tests(all_unit_tests);
+				exit(0);
+
 			case 'V':
 				printf("tmtest version %s\n", stringify(VERSION));
 				exit(0);
@@ -1181,8 +1185,6 @@
 
 int main(int argc, char **argv)
 {
-	unit_test_check(argc, argv, all_unit_tests);
-
 	orig_cwd = dup_cwd();
 	process_args(argc, argv);
 

Added: trunk/test/02-running/03-Unit-Tests.test
===================================================================
--- trunk/test/02-running/03-Unit-Tests.test	2007-01-24 02:53:28 UTC (rev 121)
+++ trunk/test/02-running/03-Unit-Tests.test	2007-01-24 03:03:51 UTC (rev 122)
@@ -0,0 +1,6 @@
+# Ensures that all unit tests pass
+
+$tmtest --run-unit-tests
+
+STDOUT:
+6 tests run, 6 successes.

Modified: trunk/zutest.c
===================================================================
--- trunk/zutest.c	2007-01-24 02:53:28 UTC (rev 121)
+++ trunk/zutest.c	2007-01-24 03:03:51 UTC (rev 122)
@@ -86,7 +86,7 @@
 {
 	run_zutest_battery(battery);
 	print_zutest_results();
-	exit(failures < 100 ? failures : 100);
+	exit(failures < 99 ? failures : 99);
 }
 
 





From tmtest-commits at lists.berlios.de  Wed Jan 24 07:58:26 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Wed, 24 Jan 2007 06:58:26 -0000
Subject: [Tmtest-commits] [123] trunk: Make REPLACE accept any number of
	substitutions to perform
Message-ID: <200701240657.l0O6v6TI013476@sheep.berlios.de>

Revision: 123
Author:   bronson
Date:     2007-01-24 07:57:05 +0100 (Wed, 24 Jan 2007)

Log Message:
-----------
Make REPLACE accept any number of substitutions to perform in a single command.

Modified Paths:
--------------
    trunk/CHANGES
    trunk/tmlib.sh
Modified: trunk/CHANGES
===================================================================
--- trunk/CHANGES	2007-01-24 03:03:51 UTC (rev 122)
+++ trunk/CHANGES	2007-01-24 06:57:05 UTC (rev 123)
@@ -1,3 +1,4 @@
+- Made REPLACE handle multiple text substitutions; could only handle 1 before.
 - Got rid of RESULT and all its code.  echo $? from within your test instead.
 - Got rid of Cutest since it was very non-cute.  Wrote zutest instead.
 - Changed --all-files to be the much more understandable --ignore-extension.

Modified: trunk/tmlib.sh
===================================================================
--- trunk/tmlib.sh	2007-01-24 03:03:51 UTC (rev 122)
+++ trunk/tmlib.sh	2007-01-24 06:57:05 UTC (rev 123)
@@ -229,12 +229,11 @@
 INDENT ()
 {
     # sed appears more binary transparent than bash's builtins so I'm
-    # using sed instead of builtin read.  It might even be faster.
-
+    # using it instead of builtin read.  It might even be faster.
     sed -e "s/^/${1-    }/"
 
-    # even though this might be faster, it mucks things up.
-    #
+    # even though it would probably be faster to do it with the
+	# Bash built-in, the following mucks things up.  Bash is sloppy.
     #	while read LINE; do
     #		echo $'\t'"$LINE"
     #	done
@@ -244,10 +243,13 @@
 # 
 # REPLACE
 #
-# Replaces all occurrences of the first argument with the second
-# argument.  Both should be regex-safe.  Use sed if you want to
-# replace with regexes.  NOTE: replace does not work if a newline
-# is embedded in either argument.
+# Replaces all occurrences of the first argument with the second argument.
+# Takes any number of arguments:
+#   REPLACE abc ABC def DEF ghi GHI
+# converts the first nine characters of the alphabet to upper case
+# All non-control characters are safe: quotes, slashes, etc.
+# Use sed if you want to replace with regexes.
+# NOTE: replace does not work if a newline is embedded in either argument.
 #
 # Three layers of escaping!  (bash, perlvar, perlre)  This is insane.
 # I wish sed or awk would work with raw strings instead of regexes.
@@ -256,28 +258,8 @@
 
 REPLACE()
 {
-    # unfortunately bash can't handle this substitution because it
-    # must work on ' and \ simultaneously.  So, send it to perl for
-    # processing.  Until ' and \ have been escaped, Perl can't 
-    
+    # unfortunately bash can't handle this substitution itself because it
+    # must work on ' and \ simultaneously. Send it to perl for processing.
 
-#    echo "got: '$1' '${1//[\'\\]/\'}'"
-#    perl -e "print \"in: \" . quotemeta('${1//\'/\'}') . \"\\n\";"
-
-#     (echo "$1"; echo "$2"; cat) | cat
-#     (echo "$1"; echo "$2"; cat) | perl -e "my \$in = <>; chomp \$in; my \$out = <>; chomp \$out; print '  in: <<' . \$in . \">>\n out: <<\" . \$out . \">>\n\"; while(<>) { print \"DATA: \$_\" }"
-#     (echo "$1"; echo "$2"; cat) | perl -e "my \$in = <>; chomp \$in; \$in=quotemeta(\$in); my \$out = <>; chomp \$out; print '  in: <<' . \$in . \">>\n out: <<\" . \$out . \">>\n\"; while(<>) { print \"DATA: \$_\" }"
-
-     (echo "$1"; echo "$2"; cat) | perl -e "my \$in = <>; chomp \$in; \$in=quotemeta(\$in); my \$out = <>; chomp \$out; while(<>) { s/\$in/\$out/g; print or die \"Could not print: \$!\\\\n\"; }"
-
-
-# this scheme also works but it's much better to feed the vars on stdin
-# along with the data.  Less process overhead, simpler script.  This
-# does mean that REPLACE won't work with embedded newlines though.
-#
-#    local in=$(perl -p -e "s/([\\'\\\\])/\\\\\$1/g" <<< $1);
-#    local out=$(perl -p -e "s/([\\'\\\\])/\\\\\$1/g" <<< $2);
-#
-#     perl -e "<>; my \$in = quotemeta(chomp); <>; my \$out = chomp; while(<>) { s/\$in/\$out/g; print or die \"Could not print: \$!\\\\n\"; }"
-#    perl -e "my \$in = quotemeta('${1//\'/\'}'); my \$out = '${2//\'/\'}'; while(<>) { s/\$in/\$out/g; print or die \"Could not print: \$!\\\\n\"; }"
+     ( while [ "$1" != "" ]; do echo "$1"; shift; done; echo; cat) | perl -e "my %ops; while(<>) { chomp; last if \$_ eq ''; \$_ = quotemeta(\$_); \$ops{\$_} = <>; chomp(\$ops{\$_}); warn 'odd number of arguments to REPLACE', last if \$ops{\$_} eq ''; } while(<>) { for my \$k (keys %ops) { s/\$k/\$ops{\$k}/g } print or die \"REPLACE: Could not print: \$!\\\\n\"; }"
 }





From tmtest-commits at lists.berlios.de  Wed Jan 24 08:46:55 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Wed, 24 Jan 2007 07:46:55 -0000
Subject: [Tmtest-commits] [124] trunk: Added the tmtest -f command-line
	option
Message-ID: <200701240745.l0O7jZAm015082@sheep.berlios.de>

Revision: 124
Author:   bronson
Date:     2007-01-24 08:45:34 +0100 (Wed, 24 Jan 2007)

Log Message:
-----------
Added the tmtest -f command-line option
Also fixed up some old tests that were conflicting with -e.

Modified Paths:
--------------
    trunk/BUGS
    trunk/CHANGES
    trunk/TODO
    trunk/main.c
    trunk/test/00-cmdline/10-CfgDir.test
    trunk/test/00-cmdline/12-CfgEmpty.test
    trunk/test.c
    trunk/test.h
    trunk/tmtest.pod

Added Paths:
-----------
    trunk/test/03-results/40-TestFailures.test
Modified: trunk/BUGS
===================================================================
--- trunk/BUGS	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/BUGS	2007-01-24 07:45:34 UTC (rev 124)
@@ -13,8 +13,8 @@
 Bash3 has removed the ability to set LINENO, so you may see the
 wrong line number be printed if there's an error.  If you want to
 see the correct line numbers, use Bash2.  It's an unfortunate
-regression but since it only affects tests scripts being piped
-on stdin it won't affect many people.
+regression.  I've worked around it so that it only affects tests
+piped in on stdin.
 
 
 The <<-EOL operator strips ALL tabs from the front of each line.
@@ -53,3 +53,8 @@
 Why is the unset command totally unrelated to the set command?
 
 Why can I "exec > file" but I can't "exec | prog"?
+	I have to exec > >(prog).  Seems asymmetrical.
+
+Why oh why can't I get the pid of the most recent process substitution?
+	(exec > >(tr a-z A-Z); pid=$!; echo howdy; wait $pid)
+	Except that $! is not set for process substitutions.

Modified: trunk/CHANGES
===================================================================
--- trunk/CHANGES	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/CHANGES	2007-01-24 07:45:34 UTC (rev 124)
@@ -1,3 +1,4 @@
+- Added -f command-line option to print only the failed tests.
 - Made REPLACE handle multiple text substitutions; could only handle 1 before.
 - Got rid of RESULT and all its code.  echo $? from within your test instead.
 - Got rid of Cutest since it was very non-cute.  Wrote zutest instead.

Modified: trunk/TODO
===================================================================
--- trunk/TODO	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/TODO	2007-01-24 07:45:34 UTC (rev 124)
@@ -1,6 +1,5 @@
 0.96:
-- Add a cmdline option that prints only tests that fail, and prints their
-  full path.
+- Change exit value.
 - Is it possible to separate STDOUT and STDERR?  Maybe stderr comes first
   in the testfile with each line prefixed by :.  Then STDOUT.  No need
   for this delimiter craziness.

Modified: trunk/main.c
===================================================================
--- trunk/main.c	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/main.c	2007-01-24 07:45:34 UTC (rev 124)
@@ -42,7 +42,8 @@
 enum {
     outmode_test,
     outmode_dump,
-    outmode_diff
+    outmode_diff,
+	outmode_failures_only
 };
 
 int outmode = outmode_test;
@@ -395,6 +396,29 @@
 }
 
 
+/* Prints the relative path from the original cwd to the current testfile */
+
+static void print_test_path(struct test *test)
+{
+	char result[PATH_MAX];
+
+	int keep = curpush(test->testfilename);
+	if(keep <= 0) {
+		printf("print_test_path: path is too long!\n");
+		return;
+	}
+
+	if(abs2rel(curabsolute(), orig_cwd, result, sizeof(result))) {
+		printf("%s\n", result);
+	} else {
+		printf("print_test_path: abs2rel error: %s relto %s\n",
+			curabsolute(), orig_cwd);
+	}
+
+	curpop(keep);
+}
+
+
 /** Runs the named testfile.
  *
  * If warn_suffix is true and the ffilename doesn't end in ".test"
@@ -451,6 +475,7 @@
     // initialize the test mode
     switch(outmode) {
         case outmode_test:
+		case outmode_failures_only:
             // nothing to do
             break;
         case outmode_dump:
@@ -561,6 +586,11 @@
             case outmode_test:
                 test_results(&test, dispname);
                 break;
+			case outmode_failures_only:
+				if(check_for_failure(&test, dispname)) {
+					print_test_path(&test);
+				}
+				break;
             case outmode_dump:
                 dump_results(&test);
                 break;
@@ -1017,6 +1047,7 @@
 		{"config", 1, 0, 'c'},
 		{"diff", 0, 0, 'd'},
 		{"dump-script", 0, &dumpscript, 1},
+		{"failures-only", 0, 0, 'f'},
 		{"help", 0, 0, 'h'},
 		{"output", 0, 0, 'o'},
 		{"quiet", 0, 0, 'q'},
@@ -1050,6 +1081,10 @@
                 outmode = outmode_diff;
                 break;
 
+			case 'f':
+				outmode = outmode_failures_only;
+				break;
+
 			case 'h':
 				usage();
 				exit(0);

Modified: trunk/test/00-cmdline/10-CfgDir.test
===================================================================
--- trunk/test/00-cmdline/10-CfgDir.test	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/test/00-cmdline/10-CfgDir.test	2007-01-24 07:45:34 UTC (rev 124)
@@ -1,3 +1,9 @@
+# Test opening a directory as if it were a config file.
+
+# Our tests run with -e by default.  Therefore, if we want to see
+# the test result, if it's nonzero, we need to turn that off.
+set +e
+
 $tmtest --config=/tmp - <<-EOL
 	echo Test!
 EOL

Modified: trunk/test/00-cmdline/12-CfgEmpty.test
===================================================================
--- trunk/test/00-cmdline/12-CfgEmpty.test	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/test/00-cmdline/12-CfgEmpty.test	2007-01-24 07:45:34 UTC (rev 124)
@@ -1,3 +1,10 @@
+# Test specifying nothing for config
+
+# tmtest runs -e by default, so we need to turn that off if we
+# want to be able to read the result of the command (else the
+# script just exits immediately and the echo never fires).
+set +e
+
 $tmtest --config= - <<-EOL
 	echo Howdy
 	STDOUT:

Added: trunk/test/03-results/40-TestFailures.test
===================================================================
--- trunk/test/03-results/40-TestFailures.test	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/test/03-results/40-TestFailures.test	2007-01-24 07:45:34 UTC (rev 124)
@@ -0,0 +1,52 @@
+# ensures that tmtest -f works.
+#
+# We verify that it runs all tests and only prints the ones that fail.
+# We hit it with a disabled test, an aborted test, and 
+
+MKDIR dir
+
+MKFILE fsuccess "$dir/success.test" <<-EOL
+	# ensure this test isn't printed
+	echo HI
+	STDOUT:
+	HI
+EOL
+
+MKFILE ffail "$dir/fail.test" <<-EOL
+	# ensure this test is printed
+	echo HI
+	STDOUT:
+	HO
+EOL
+
+MKFILE fdisabled "$dir/disabled.test" <<-EOL
+	# this test would succeed if it weren't disabled
+	DISABLED
+	echo HI
+	STDOUT:
+	HI
+EOL
+
+# We can't do this because it stops the entire test run.
+#MKFILE fabort "$dir/aborted.test" <<-EOL
+#	# this test would succeed if it didn't abort
+#	ABORT
+#	echo HI
+#	STDOUT:
+#	HI
+#EOL
+
+MKDIR subdir "$dir/subdir"
+
+MKFILE subfail "$subdir/fail.test" <<-EOL
+	# make sure we show the proper path in subdirectories
+	echo HI
+	STDOUT:
+	HO
+EOL
+
+$tmtest -f $dir | REPLACE `basename "$dir"` DIR
+
+STDOUT:
+DIR/fail.test
+DIR/subdir/fail.test

Modified: trunk/test.c
===================================================================
--- trunk/test.c	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/test.c	2007-01-24 07:45:34 UTC (rev 124)
@@ -705,30 +705,24 @@
 }
 
 
-/** Checks the actual results against the expected results.
- * dispname is the name we should display for the test.
- */
-
-void test_results(struct test *test, const char *dispname)
+static void test_analyze_results(struct test *test, int *stdo, int *stde)
 {
     scanstate scanner;
     char scanbuf[BUFSIZ];
-	int stdo, stde;	// true if there are differences.
 	
+	*stdo = *stde = -1;
+
 	if(was_aborted(test->status)) {
-		print_reason(test, "ABRT", "by");
 		test_failures++;
 		test->aborted = 1;
 		return;
 	}
 
 	if(was_disabled(test->status)) {
-		print_reason(test, "dis ", "by");
 		return;
 	}
 
     if(!was_started(test->status)) {
-		print_reason(test, "ERR ", "error in");
         test_failures++;
         return;
     }
@@ -749,14 +743,45 @@
         test->stderr_match = (fd_has_data(test->errfd) ? match_no : match_yes);
     }
 
-    stdo = (test->stdout_match != match_yes);
-    stde = (test->stderr_match != match_yes);
+    *stdo = (test->stdout_match != match_yes);
+    *stde = (test->stderr_match != match_yes);
 
+    if(!*stdo && !*stde && !test->exitsignal) {
+        test_successes++;
+    } else {
+        test_failures++;
+	}
+}
+
+
+/** Checks the actual results against the expected results.
+ * dispname is the name we should display for the test.
+ */
+
+void test_results(struct test *test, const char *dispname)
+{
+	int stdo, stde;	// true if there are differences.
+
+	test_analyze_results(test, &stdo, &stde);
+	
+	if(test->aborted) {
+		print_reason(test, "ABRT", "by");
+		return;
+	}
+
+	if(was_disabled(test->status)) {
+		print_reason(test, "dis ", "by");
+		return;
+	}
+
+    if(!was_started(test->status)) {
+		print_reason(test, "ERR ", "error in");
+        return;
+    }
+
     if(!stdo && !stde && !test->exitsignal) {
-        test_successes++;
         printf("ok   %s \n", convert_testfile_name(dispname));
     } else {
-        test_failures++;
         printf("FAIL %-25s ", convert_testfile_name(dispname));
         if(test->exitsignal) {
             printf("terminated by signal %d%s", test->exitsignal,
@@ -779,6 +804,24 @@
 }
 
 
+/** Like test_results() except that it returns 1 if the test failed
+ *  and 0 if it was disabled or succeeded.
+ */
+
+int check_for_failure(struct test *test, const char *testpath)
+{
+	int stdo, stde;	// true if there are differences.
+	int oldfailures = test_failures;
+
+	test_analyze_results(test, &stdo, &stde);
+	if(oldfailures+1 == test_failures) {
+		return 1;
+	}
+
+	return 0;
+}
+
+
 /**
  * Reads all the data from infd and writes it onto outfd.
  *

Modified: trunk/test.h
===================================================================
--- trunk/test.h	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/test.h	2007-01-24 07:45:34 UTC (rev 124)
@@ -85,6 +85,7 @@
 void test_results(struct test *test, const char *dispname);
 void dump_results(struct test *test);
 void print_test_summary(struct timeval *start, struct timeval *stop);
+int check_for_failure(struct test *test, const char *testpath);
 
 void test_init(struct test *test);
 void test_free(struct test *test);

Modified: trunk/tmtest.pod
===================================================================
--- trunk/tmtest.pod	2007-01-24 06:57:05 UTC (rev 123)
+++ trunk/tmtest.pod	2007-01-24 07:45:34 UTC (rev 124)
@@ -8,8 +8,6 @@
 
 =head1 DESCRIPTION
 
-
-
 =head1 OPTIONS
 
 =over 8
@@ -41,6 +39,12 @@
 into your test deck.  Make sure you know exactly what you
 changed, right down to the whitespace.
 
+=item B<-f> B<--failures-only>
+
+Runs the given tests and prints the paths of the tests that fail.
+If you're struggling with a few failing tests, this will give you
+a concise report of exactly what testfiles need to be investigated.
+
 =item B<--ignore-extension>
 
 Normally tmtest only runs files with names that end in ".test".
@@ -55,10 +59,6 @@
 
 =back
 
-=head1 TESTFILE
-
-=over 8
-
 =head1 CONFIGURATION
 
 tmtest reads its configuration first from F</etc/tmtest.conf> and
@@ -107,23 +107,59 @@
 could not be run for some reason.  If all tests are successfully run,
 even if they all fail, tmtest returns 0.
 
-=head1 SUGGESTIONS
+TODO: this is an anachronism.  It should return the number of failing
+tests or 99, whichever is less.  If no tests fail then it returns
+of course 0.
 
+=head1 TESTFILE
+
+=over 8
+
+=back
+
+=head1 TEST RESULTS
+
+There are two schools of thought on how testfiles should be written:
+- like a shell script, where you need to manually check after the
+commands that matter if they failed or not.
+
+	command
+	[ $? != 0 ] && echo got $? from command && exit $?
+
+- like a makefile, where the first command that returns a nonzero
+exit status causes the whole process to bail out.
+
+	command
+	# no need to check; if it returned nonzero, the script already exited.
+
+tmtest uses the first technique by default; it treats testfiles like
+shell scripts.
+
+To make your testfile bail the instant it encounters an error, add
+the command "set -e" to it.  If any subsequent command returns an
+error, everything will bail out.  Add "set +e" to turn this behavior
+back off.  You may also want to "set -o pipefail" as well.
+
+If your testfiles are trivial, it probably makes sens to use -e to
+save yourself a lot of manual checking.  Otherwise, the default of
++e is probably the safest, most understandable way to write tests.
+
+=head1 TESTFILE SUGGESTIONS
+
 Look in the "examples" directory that came with this distribution.
 You may find some helpful utilities.
 
-If you want to be more pedantic, you might want to "set -e" in your
-test file or configuration file.  This will make it so the test will
-stop at the very first command that returns a nonzero exit status.
-
 If you can't figure out what your test is doing, you might want
-to add "set -v" or "set -x" to the top of the testfile.  This will
-show you line-by-line what is happening.  You can also put them in
+to add "set -x" to the top of the testfile.  This will
+show you line-by-line what is happening (use "set -v" if "set -x"'s
+command-line substitution produces hard-to-read results).
+You can also put them in
 a config file if you're having configuration issues.
 
 Never create files in /tmp with static or easily predictable file names.
 This opens you up to symlink attacks.  Instead, use mktemp(1)
-or tempfile(1).  Also check the examples directory for functions
+or tempfile(1) (tmtest's built-in helpers use mktemp).
+Also check the examples directory for functions
 to make working with files easier.
 
 	file=`mktemp -t 02-XXXXXX` || ABORT can't mktemp.





From tmtest-commits at lists.berlios.de  Thu Jan 25 00:52:35 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Wed, 24 Jan 2007 23:52:35 -0000
Subject: [Tmtest-commits] [125] trunk: Remove all exit() calls from all
	files except for main.c.
Message-ID: <200701242351.l0ONpEvF008396@sheep.berlios.de>

Revision: 125
Author:   bronson
Date:     2007-01-25 00:51:11 +0100 (Thu, 25 Jan 2007)

Log Message:
-----------
Remove all exit() calls from all files except for main.c.  Use test_abort().

Modified Paths:
--------------
    trunk/TODO
    trunk/compare.c
    trunk/main.c
    trunk/test/00-cmdline/10-CfgDir.test
    trunk/test/00-cmdline/12-CfgEmpty.test
    trunk/test.c
    trunk/test.h
    trunk/tmtest.pod
    trunk/vars.c
    trunk/zutest.h
Modified: trunk/TODO
===================================================================
--- trunk/TODO	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/TODO	2007-01-24 23:51:11 UTC (rev 125)
@@ -1,12 +1,12 @@
 0.96:
-- Change exit value.
+- Clean up zutest.h.
 - Is it possible to separate STDOUT and STDERR?  Maybe stderr comes first
   in the testfile with each line prefixed by :.  Then STDOUT.  No need
   for this delimiter craziness.
   YES, stderr ALWAYS comes first in the testfile, followed by stdout.
 - MKFILE should just create the file that the user names.
 - MKTMPFILE should create a filename and store that in the user's variable.
-- Write unit tests for path normalization.
+- Write unit tests for path normalization.  Maybe for CURPATH too.
 - Make tmtest only execute config files owned by either the user or root.
   Print a big fat warning when the config file is skipped.  This prevents
   a malicious user from putting a config file in /tmp and having it
@@ -43,10 +43,7 @@
         // not above the given config file.  i.e. if user specifies
         // "tmtest -c /a/b/cc /a/t/u/t.test", we will look for config files
         // in /a/t/tmtest.conf and /a/t/u/tmtest.conf.
-- get rid of all the exit(10) calls in test.c.  We need a better way to
-  abort the test.
-  should get rid of a whole bunch in main.c as well.
-  this should prevent us from dropping turds in /tmp all the time too.
+- Get rid of all the exit() calls from main.c.
 - Ensure it compiles and runs on freebsd.
 - Get rid of -g, add -O2.  Make it easy to set these for compilation.
   Yes, have dev and prod modes.  dev would be -O0 and -g and include
@@ -54,6 +51,14 @@
   unit tests only add 12K or so, where might as well just leave em).
 - If there were failures, should highlight that in the test summary.
 	"%d FAILURES" or somesuch.
+- Write a sample tmtest.conf that walks you through setting -e or not,
+  indenting STDERR or not, etc.
+- Make zutest recursive: there should be no need for zutest_battery.
+  a zutest_suite should be able to contain any number of other
+  zutest_suites.
+- Add the zutest unit tests to the tmtest test battery.
+- Make zutest able to run both quiet (only failures printed) and verbose
+  (everything printed and then some).
 
 0.98:
 - Change the I/O scheme to be event based.  Get rid of the tempfiles.
@@ -64,6 +69,8 @@
   - This also allows us to get rid of STATUSFD so that NO other fds are open
     when running the test.  OUTFD and ERRFD still need to be open when
 	running the config files though.
+- Allow running more than one test at once.  Add -j option like gmake.
+  Maybe should also read MAKEFLAGS from the environment?
 - Use i/o lib for everything.  No need for temp files.
   This means that we stream everything EXCEPT stderr, which we memory
   buffer.  If your stderr is more than 100K or so in size, just redirect
@@ -101,6 +108,8 @@
   is [0-9a-zA-Z_] then it's an identifier and we'll make a tempfile.
   Otherwise, we'll use the arg as a filename.  ER, IS THIS A GOOD IDEA?
 
+rc series...
+
 1.0!
 
 1.2:

Modified: trunk/compare.c
===================================================================
--- trunk/compare.c	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/compare.c	2007-01-24 23:51:11 UTC (rev 125)
@@ -95,9 +95,7 @@
             ss->line += n;
             if(n < 0) {
                 // there was an error while trying to fill the buffer
-                // TODO: this should be propagated to the client somehow?
-                perror("compare_continue_bytes");
-                exit(10);
+				return -1;
             }
             if(n == 0) {
 				// banged into the EOF
@@ -196,6 +194,7 @@
 static void test_empty()
 {
 	scanstate ssrec, *ss=&ssrec;
+	int val;
 
 	readmem_init_str(ss, "");
 	compare_attach(ss);
@@ -203,7 +202,8 @@
 
 	readmem_init_str(ss, "");
 	compare_attach(ss);
-	compare_continue(ss, "", 0);
+	val = compare_continue(ss, "", 0);
+	AssertNonNegative(val);
 	AssertEq(compare_check(ss), cmp_full_match);
 }
 
@@ -211,11 +211,14 @@
 static void test_standard()
 {
 	scanstate ssrec, *ss=&ssrec;
+	int val;
 
 	readmem_init_str(ss, "123");
 	compare_attach(ss);
-	compare_continue(ss, "12", 2);
-	compare_continue(ss, "3", 1);
+	val = compare_continue(ss, "12", 2);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "3", 1);
+	AssertNonNegative(val);
 	AssertEq(compare_check(ss), cmp_full_match);
 }
 
@@ -225,14 +228,15 @@
 	char buf[BUFSIZ];
 	scanstate ssrec, *ss=&ssrec;
 	unsigned int seed = 47;
-	int num, i;
+	int num, i, val;
 
 	scanstate_init(ss, buf, BUFSIZ);
 	readrand_attach(ss, seed);
 	compare_attach(ss);
 	for(i=0; i<10; i++) {
 		num = rand_r(&seed);
-		compare_continue(ss, (char*)&num, sizeof(num));
+		val = compare_continue(ss, (char*)&num, sizeof(num));
+		AssertNonNegative(val);
 	}
 
 	// compare_check will never return cmp_full_match because
@@ -243,9 +247,12 @@
 
 static void test_strings(scanstate *ss, const char *s1, const char *s2)
 {
+	int val;
+
 	readmem_init(ss, s1, strlen(s1));
 	compare_attach(ss);
-	compare_continue(ss, s2, strlen(s2));
+	val = compare_continue(ss, s2, strlen(s2));
+	AssertNonNegative(val);
 }
 
 
@@ -279,19 +286,26 @@
 	// Tries to ensure that packetization won't mess us up.
 
 	scanstate ssrec, *ss=&ssrec;
+	int val;
 
 	readmem_init_str(ss, "12");
 	compare_attach(ss);
-	compare_continue(ss, "1", 1);
-	compare_continue(ss, "2", 1);
-	compare_continue(ss, "\n", 1);
+	val = compare_continue(ss, "1", 1);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "2", 1);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "\n", 1);
+	AssertNonNegative(val);
 	AssertEq(compare_check(ss), cmp_ptr_has_extra_nl);
 
 	readmem_init_str(ss, "123");
 	compare_attach(ss);
-	compare_continue(ss, "1", 1);
-	compare_continue(ss, "2", 1);
-	compare_continue(ss, "\n", 1);
+	val = compare_continue(ss, "1", 1);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "2", 1);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "\n", 1);
+	AssertNonNegative(val);
 	AssertEq(compare_check(ss), cmp_no_match);
 }
 
@@ -301,25 +315,34 @@
 	// Tries to ensure packetization won't mess up the newline checking.
 
 	scanstate ssrec, *ss=&ssrec;
+	int val;
 
 	readmem_init_str(ss, "123");
 	compare_attach(ss);
-	compare_continue(ss, "1", 1);
-	compare_continue(ss, "2", 1);
-	compare_continue(ss, "3", 1);
-	compare_continue(ss, "\n", 1);
+	val = compare_continue(ss, "1", 1);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "2", 1);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "3", 1);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "\n", 1);
+	AssertNonNegative(val);
 	AssertEq(compare_check_newlines(ss), cmp_ptr_has_extra_nl);
 
 	readmem_init_str(ss, "123\n");
 	compare_attach(ss);
-	compare_continue(ss, "1", 1);
-	compare_continue(ss, "2", 1);
-	compare_continue(ss, "3", 1);
+	val = compare_continue(ss, "1", 1);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "2", 1);
+	AssertNonNegative(val);
+	val = compare_continue(ss, "3", 1);
+	AssertNonNegative(val);
 	AssertEq(compare_check_newlines(ss), cmp_ss_has_extra_nl);
 
 	readmem_init_str(ss, "");
 	compare_attach(ss);
-	compare_continue(ss, "\n", 1);
+	val = compare_continue(ss, "\n", 1);
+	AssertNonNegative(val);
 	AssertEq(compare_check_newlines(ss), cmp_ptr_has_extra_nl);
 
 	readmem_init_str(ss, "\n");

Modified: trunk/main.c
===================================================================
--- trunk/main.c	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/main.c	2007-01-24 23:51:11 UTC (rev 125)
@@ -83,7 +83,7 @@
 // exit values:
 enum {
     no_error = 0,
-    argument_error,
+    argument_error=100,
     runtime_error,
     interrupted_error,
     internal_error,
@@ -268,7 +268,7 @@
     int fd = open(fn, flags|O_RDWR|O_CREAT/*|O_EXCL*/, S_IRUSR|S_IWUSR);
     if(fd < 0) {
         fprintf(stderr, "couldn't open %s: %s\n", fn, strerror(errno));
-        exit(runtime_error);	// TODO
+        exit(runtime_error);
     }
 
     return fd;
@@ -283,13 +283,13 @@
 	buf = malloc(sizeof(TESTDIR) + sizeof(DIFFNAME));
 	if(!buf) {
 		perror("malloc");
-		exit(10);	// TODO
+		exit(runtime_error);
 	}
 
 	test->diffname = buf;
 	fd = open_file(buf, DIFFNAME, 0);
 	assert(strlen(buf) == sizeof(TESTDIR)+sizeof(DIFFNAME)-1);
-	write_file(fd, 0, NULL);
+	write_file(test, fd, 0, NULL);
 	close(fd);
 
 	return fd;
@@ -449,6 +449,7 @@
     int fd = -1;
     int i;
     FILE *tochild;
+	jmp_buf abort_jump;
 
     // defined in the exec.c file generated by exec.tmpl.
     extern const char exec_template[];
@@ -467,6 +468,12 @@
 	}
 
     test_init(&test);
+	if(setjmp(abort_jump)) {
+		// test was aborted.
+		fprintf(stderr, "Test aborted: %s\n", test.status_reason);
+        exit(runtime_error);
+	}
+
     test.testfilename = name;
     test.outfd = g_outfd;
     test.errfd = g_errfd;
@@ -547,7 +554,7 @@
         if(fd < 0) {
             fprintf(stderr, "Could not open %s: %s\n",
                     curabsolute(), strerror(errno));
-            exit(runtime_error); // TODO
+            exit(runtime_error);
         }
         readfd_attach(&test.testfile, fd);
     }
@@ -575,7 +582,7 @@
         i = wait_for_child(child, "test");
         test.exitsignal = (WIFSIGNALED(i) ? WTERMSIG(i) : 0);
         test.exitcored = (WIFSIGNALED(i) ? WCOREDUMP(i) : 0);
-        // test.exitno = (WIFEXITED(i) ? WEXITSTATUS(i) : 256);
+        test.exitno = (WIFEXITED(i) ? WEXITSTATUS(i) : 256);
 
         // read the status file to determine what happened
         // and store the information in the test struct.
@@ -609,7 +616,7 @@
             close(fd);
         }
 
-        keepontruckin = !test.aborted;
+        keepontruckin = !was_aborted(test.status);
     }
 
     test_free(&test);
@@ -1239,6 +1246,6 @@
     }
 
 	free((char*)orig_cwd);
-	return 0;
+	return test_get_exit_value();
 }
 

Modified: trunk/test/00-cmdline/10-CfgDir.test
===================================================================
--- trunk/test/00-cmdline/10-CfgDir.test	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/test/00-cmdline/10-CfgDir.test	2007-01-24 23:51:11 UTC (rev 125)
@@ -12,4 +12,4 @@
 STDERR:
 Could not open /tmp: not a file!
 STDOUT:
-RESULT: 2
+RESULT: 101

Modified: trunk/test/00-cmdline/12-CfgEmpty.test
===================================================================
--- trunk/test/00-cmdline/12-CfgEmpty.test	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/test/00-cmdline/12-CfgEmpty.test	2007-01-24 23:51:11 UTC (rev 125)
@@ -15,4 +15,4 @@
 STDERR:
 You must specify a directory for --config.
 STDOUT:
-RESULT: 1
+RESULT: 100

Modified: trunk/test.c
===================================================================
--- trunk/test.c	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/test.c	2007-01-24 23:51:11 UTC (rev 125)
@@ -17,6 +17,7 @@
 #include <errno.h>
 #include <dirent.h>
 #include <assert.h>
+#include <stdarg.h>
 
 #include "re2c/read-fd.h"
 
@@ -57,12 +58,11 @@
  * Actually, it just returns the file's length.
  */
 
-int fd_has_data(int fd)
+int fd_has_data(struct test *test, int fd)
 {
     off_t pos = lseek(fd, 0, SEEK_END);
     if(pos < 0) {
-        perror("lseek in fd_has_data");
-        exit(10);   // todo: consolidate with error codes in main
+		test_abort(test, "fd_has_data lseek error: %s", strerror(errno));
     }
 
     return pos;
@@ -146,8 +146,8 @@
 
     // first rewind the status file
     if(lseek(test->statusfd, 0, SEEK_SET) < 0) {
-        fprintf(stderr, "read_file lseek for status file: %s\n", strerror(errno));
-        exit(10);   // todo: consolidate with error codes in main
+        test_abort(test, "read_file lseek on status file: %s\n",
+			strerror(errno));
     }
 
     // then create our scanner
@@ -163,9 +163,8 @@
 
 		// look for errors...
         if(tok < 0) {
-            fprintf(stderr, "Error %d pulling status tokens: %s\n", 
-                    tok, strerror(errno));
-            exit(10);
+            test_abort(test, "Error %d pulling status tokens: %s\n",
+				tok, strerror(errno));
         } else if(tok == stGARBAGE) {
 			fprintf(stderr, "Garbage on line %d in the status file: '%.*s'\n",
 					ss.line, (int)token_length(&ss)-1, token_start(&ss));
@@ -292,9 +291,8 @@
         oldline = test->testfile.line;
         int tokno = scan_next_token(&test->testfile);
         if(tokno < 0) {
-            fprintf(stderr, "Error %d pulling status tokens: %s\n", 
+            test_abort(test, "Error %d pulling status tokens: %s\n", 
                     tokno, strerror(errno));
-            exit(10);
         } else if(tokno == 0) {
 			// if the test file is totally empty.
 			break;
@@ -336,13 +334,14 @@
  * it up.
  */
 
-void compare_section_start(scanstate *cmpscan, int fd,
-		const char *filename, const char *sectionname)
+void compare_section_start(struct test *test,
+	scanstate *cmpscan, int fd,
+	const char *sectionname)
 {
     // rewind the file
     if(lseek(fd, 0, SEEK_SET) < 0) {
-        fprintf(stderr, "read_file lseek for status file: %s\n", strerror(errno));
-        exit(10);   // todo: consolidate with error codes in main
+        test_abort(test, "compare_section_start lseek compare: %s\n",
+			strerror(errno));
     }
 
     scanstate_reset(cmpscan);
@@ -487,8 +486,7 @@
     }
 
 	scanstate_reset(cmpscan);
-    compare_section_start(cmpscan, fd,
-        get_testfile_name(test), secname);
+    compare_section_start(test, cmpscan, fd, secname);
 
 	// store the newline flag in the cmpscan structure
 	cmpscan_suppress_newline = suppress_trailing_newline;
@@ -585,6 +583,7 @@
 {
     // cmpscan is the scanner used to perform the diff.
     scanstate *cmpscan = refcon;
+	int val;
 
     // the section that we're processing (without the NEW flag attached)
     int newsec = EX_TOKEN(sec);
@@ -639,7 +638,10 @@
                 break;
             case exSTDOUT:
             case exSTDERR:
-				compare_continue(cmpscan, datap, len);
+				val = compare_continue(cmpscan, datap, len);
+				if(val < 0) {
+					test_abort(test, "compare_continue error: %d\n", val);
+				}
                 break;
             case exCOMMAND:
                 break;
@@ -672,9 +674,8 @@
     do {
         int tokno = scan_next_token(scanner);
         if(tokno < 0) {
-            fprintf(stderr, "Error %d pulling status tokens: %s\n", 
+            test_abort(test, "scan_sections error %d pulling status tokens: %s\n", 
                     tokno, strerror(errno));
-            exit(10);
         } else if(tokno == 0) {
 			break;
 		}
@@ -714,7 +715,6 @@
 
 	if(was_aborted(test->status)) {
 		test_failures++;
-		test->aborted = 1;
 		return;
 	}
 
@@ -737,10 +737,10 @@
     assert(test->stderr_match != match_inprogress);
 
     if(test->stdout_match == match_unknown) {
-        test->stdout_match = (fd_has_data(test->outfd) ? match_no : match_yes);
+        test->stdout_match = (fd_has_data(test, test->outfd) ? match_no : match_yes);
     }
     if(test->stderr_match == match_unknown) {
-        test->stderr_match = (fd_has_data(test->errfd) ? match_no : match_yes);
+        test->stderr_match = (fd_has_data(test, test->errfd) ? match_no : match_yes);
     }
 
     *stdo = (test->stdout_match != match_yes);
@@ -764,7 +764,7 @@
 
 	test_analyze_results(test, &stdo, &stde);
 	
-	if(test->aborted) {
+	if(was_aborted(test->status)) {
 		print_reason(test, "ABRT", "by");
 		return;
 	}
@@ -830,7 +830,7 @@
  * @returns the number of bytes written.
  */
 
-size_t write_file(int outfd, int infd, int *endnl)
+size_t write_file(struct test *test, int outfd, int infd, int *endnl)
 {
     char buf[BUFSIZ];
     size_t rcnt, wcnt;
@@ -838,8 +838,7 @@
 
     // first rewind the input file
     if(lseek(infd, 0, SEEK_SET) < 0) {
-        fprintf(stderr, "write_file lseek on %d: %s\n", infd, strerror(errno));
-        exit(10);   // todo: consolidate with error codes in main
+		test_abort(test, "write_file lseek on %d: %s\n", infd, strerror(errno));
     }
 
     // then write the file.
@@ -853,14 +852,14 @@
                 wcnt = write(outfd, buf, rcnt);
             } while(wcnt < 0 && errno == EINTR);
             if(wcnt < 0) {
-                // write error.  do something!
-                perror("writing in write_file");
+                test_abort(test, "write_file got %s while writing!",
+					strerror(errno));
                 break;
             }
 			total += rcnt;
         } else if (rcnt < 0) {
-            // read error.  do something!
-            perror("reading in write_file");
+			test_abort(test, "write_file got %s while reading!",
+				strerror(errno));
             break;
         }
     } while(rcnt);
@@ -881,7 +880,7 @@
 			start_output_section_argproc, &marked_no_nl);
 
 	write(test->rewritefd, datap, len);
-	cnt = write_file(test->rewritefd, fd, &has_nl);
+	cnt = write_file(test, test->rewritefd, fd, &has_nl);
 
 	if(marked_no_nl) {
 		// if a section is marked with --no-trailing-newline, we need
@@ -966,7 +965,6 @@
 
 	if(was_aborted(test->status)) {
 		dump_reason(test, "was aborted");
-		test->aborted = 1;
 		return;
 	}
 
@@ -995,13 +993,13 @@
 
     // if any sections haven't been output, but they differ from
     // the default, then they need to be output here at the end.
-    if(test->stderr_match == match_unknown && fd_has_data(test->errfd)) {
+    if(test->stderr_match == match_unknown && fd_has_data(test, test->errfd)) {
 		write_strconst(test->rewritefd, "STDERR:\n");
-        write_file(test->rewritefd, test->errfd, NULL);
+        write_file(test, test->rewritefd, test->errfd, NULL);
     }
-    if(test->stdout_match == match_unknown && fd_has_data(test->outfd)) {
+    if(test->stdout_match == match_unknown && fd_has_data(test, test->outfd)) {
 		write_strconst(test->rewritefd, "STDOUT:\n");
-        write_file(test->rewritefd, test->outfd, NULL);
+        write_file(test, test->rewritefd, test->outfd, NULL);
     }
 }
 
@@ -1031,6 +1029,21 @@
 }
 
 
+void test_abort(struct test *test, const char *fmt, ...)
+{
+	char buf[BUFSIZ];
+	va_list ap;
+
+	va_start(ap, fmt);
+	vsnprintf(buf, sizeof(buf), fmt, ap);
+	va_end(ap);
+
+	test->status = test_was_aborted;
+	test->status_reason = strdup(buf);
+	longjmp(test->abort_jump, 1);
+}
+
+
 void test_free(struct test *test)
 {
 	int err;
@@ -1057,3 +1070,8 @@
 }
 
 
+int test_get_exit_value()
+{
+	return test_failures < 99 ? test_failures : 99;
+}
+

Modified: trunk/test.h
===================================================================
--- trunk/test.h	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/test.h	2007-01-24 23:51:11 UTC (rev 125)
@@ -7,6 +7,7 @@
  */
 
 #include "compare.h"
+#include <setjmp.h>
 
 
 /**
@@ -31,7 +32,7 @@
 
 	test_was_started=16,	///< test was started but we haven't received an exit status yet.
 	test_was_completed,		///< test completed normally.  tests may abort prematurely but still consider it a successful run, so use test_was_started.  this status is largely useless.
-	test_was_aborted,		///< somebody called abort in the middle of the test.
+	test_was_aborted,		///< somebody called abort in the middle of the test
 	test_was_disabled,		///< the test was disabled by somebody.
 } test_status;
 
@@ -60,6 +61,7 @@
     int outfd;				///< the file that receives the test's stdout.
     int errfd;				///< the file that receives the test's stderr.
     int statusfd;			///< receives the runtime test status messages.
+	int exitno;				///< the testfile exited with this value
     int exitsignal;         ///< the value returned for the test by waitpid(2)
     int exitcored;          ///< if exitsignal is true, true if child core dumped.
 
@@ -72,10 +74,11 @@
 
 	int num_config_files;	///< the number of config files we started processing.  If the status is higher than test_was_started, then this gives the total number of config files processed.
 	char *last_file_processed; ///< if it could be discovered, this contains the name of the last file to be started.  must be freed.
-	int aborted;			///< true if the test was aborted (and therefore no further tests should be run).
 
     enum matchval stdout_match;	///< tells whether the expected and actual stdout matches.
     enum matchval stderr_match;	///< tells whether the expected and actual stderr matches.
+
+	jmp_buf abort_jump;
 };
 
 
@@ -86,12 +89,14 @@
 void dump_results(struct test *test);
 void print_test_summary(struct timeval *start, struct timeval *stop);
 int check_for_failure(struct test *test, const char *testpath);
+int test_get_exit_value();
 
 void test_init(struct test *test);
 void test_free(struct test *test);
+void test_abort(struct test *test, const char *fmt, ...);
 
 
 // random utility function for start_diff.  Return value is true if the
 // file ends in a newline, false if not.
-size_t write_file(int outfd, int infd, int *ending_nl);
+size_t write_file(struct test *test, int outfd, int infd, int *ending_nl);
 

Modified: trunk/tmtest.pod
===================================================================
--- trunk/tmtest.pod	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/tmtest.pod	2007-01-24 23:51:11 UTC (rev 125)
@@ -103,13 +103,13 @@
 
 =head1 EXIT VALUE
 
-tmtest returns a nonzero error code only if one or more tests
-could not be run for some reason.  If all tests are successfully run,
-even if they all fail, tmtest returns 0.
+tmtest exits with the number of tests that failed or were aborted.
+If the entire test deck succeeds or is disabled, tmtest returns 0.
+If more than 99 tests fail, tmtest just returns 99.
 
-TODO: this is an anachronism.  It should return the number of failing
-tests or 99, whichever is less.  If no tests fail then it returns
-of course 0.
+Generally, if something else prevented tests from running (say,
+lack of memory or disk space), tmtest will return a number
+between 100 and 199.  TODO: this isn't true yet.
 
 =head1 TESTFILE
 

Modified: trunk/vars.c
===================================================================
--- trunk/vars.c	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/vars.c	2007-01-24 23:51:11 UTC (rev 125)
@@ -106,7 +106,7 @@
 /** Returns the full path to the user's home directory.
  */
 
-static char* get_home_dir()
+static char* get_home_dir(struct test *test)
 {
 	char *cp;
 	struct passwd *entry;
@@ -122,8 +122,8 @@
 		if(cp) return cp;
 	}
 
-	fprintf(stderr, "Could not locate your home directory!\n");
-	exit(10);
+	test_abort(test, "Could not locate your home directory!  Please set $HOME.\n");
+	return NULL; // will never be executed
 }
 
 
@@ -215,8 +215,7 @@
 		buf[sizeof(buf)-1] = '\0';
 		cp = strrchr(buf, '/');
 		if(cp == NULL) {
-			fprintf(stderr, "Illegal config_file: '%s'\n", buf); 
-			exit(1);
+			test_abort(test, "Illegal config_file: '%s'\n", buf); 
 		}
 		*cp = '\0';
 		check_config_str(test, fp, buf, cp+1);
@@ -224,7 +223,7 @@
 	} else {
 		check_config_str(test, fp, "/etc", CONFIG_FILE);
 		check_config_str(test, fp, "/etc/tmtest", CONFIG_FILE);
-		check_config_str(test, fp, get_home_dir(), HOME_CONFIG_FILE);
+		check_config_str(test, fp, get_home_dir(test), HOME_CONFIG_FILE);
 	}
 
 	// check config files in the current hierarchy

Modified: trunk/zutest.h
===================================================================
--- trunk/zutest.h	2007-01-24 07:45:34 UTC (rev 124)
+++ trunk/zutest.h	2007-01-24 23:51:11 UTC (rev 125)
@@ -27,6 +27,9 @@
 #define AssertGe(x,y) AssertOp(x,y,>=)
 #define AssertLt(x,y) AssertOp(x,y,<)
 #define AssertLe(x,y) AssertOp(x,y,<=)
+#define AssertPositive(x) AssertOp(x,0,>)
+#define AssertNegative(x) AssertOp(x,0,<)
+#define AssertNonNegative(x) AssertOp(x,0,>=)
 #define AssertIntEq(x,y) AssertOp(x,y,==)
 #define AssertIntNe(x,y) AssertOp(x,y,!=)
 #define AssertIntGt(x,y) AssertOp(x,y,>)





From tmtest-commits at lists.berlios.de  Thu Jan 25 02:04:11 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Thu, 25 Jan 2007 01:04:11 -0000
Subject: [Tmtest-commits] [126] trunk: Updated to the newest zutest
Message-ID: <200701250102.l0P12nvm022612@sheep.berlios.de>

Revision: 126
Author:   bronson
Date:     2007-01-25 02:02:48 +0100 (Thu, 25 Jan 2007)

Log Message:
-----------
Updated to the newest zutest

Modified Paths:
--------------
    trunk/Makefile
    trunk/TODO
    trunk/test/02-running/03-Unit-Tests.test
    trunk/zutest.c
    trunk/zutest.h

Removed Paths:
-------------
    trunk/zutest-tests.c
Modified: trunk/Makefile
===================================================================
--- trunk/Makefile	2007-01-24 23:51:11 UTC (rev 125)
+++ trunk/Makefile	2007-01-25 01:02:48 UTC (rev 126)
@@ -122,5 +122,5 @@
 reupdate:
 	ls re2c/*.[ch] | (ODIR=`pwd`; cd ../oe; xargs cp --target-directory $$ODIR/re2c)
 
-zutest: zutest.c zutest-tests.c zutest.h
-	gcc -Wall -Werror -g zutest.c zutest-tests.c -o zutest
+zutest: zutest.c zutest.h Makefile
+	gcc -Wall -Werror -g zutest.c -DZUTEST_MAIN -o zutest

Modified: trunk/TODO
===================================================================
--- trunk/TODO	2007-01-24 23:51:11 UTC (rev 125)
+++ trunk/TODO	2007-01-25 01:02:48 UTC (rev 126)
@@ -1,5 +1,4 @@
 0.96:
-- Clean up zutest.h.
 - Is it possible to separate STDOUT and STDERR?  Maybe stderr comes first
   in the testfile with each line prefixed by :.  Then STDOUT.  No need
   for this delimiter craziness.

Modified: trunk/test/02-running/03-Unit-Tests.test
===================================================================
--- trunk/test/02-running/03-Unit-Tests.test	2007-01-24 23:51:11 UTC (rev 125)
+++ trunk/test/02-running/03-Unit-Tests.test	2007-01-25 01:02:48 UTC (rev 126)
@@ -3,4 +3,4 @@
 $tmtest --run-unit-tests
 
 STDOUT:
-6 tests run, 6 successes.
+All OK.  6 tests run, 6 successes (57 assertions).

Deleted: trunk/zutest-tests.c
===================================================================
--- trunk/zutest-tests.c	2007-01-24 23:51:11 UTC (rev 125)
+++ trunk/zutest-tests.c	2007-01-25 01:02:48 UTC (rev 126)
@@ -1,242 +0,0 @@
-/* zutest.c
- * Scott Bronson
- * 6 Mar 2006
- *
- * Runs a bunch of unit tests.  Ensures each macro expands without error
- * and passes a simple test.  Does not test each macro for all failure
- * modes because that would be a hell of a lot of code.
- *
- * A certain number of tests are expected to fail.  If the name of a
- * failing test does not end in "_fail" then it is NOT expected to fail.
- */
-
-#include "zutest.h"
-#include <string.h>
-
-
-void test_fail()
-{
-	Fail("Gone!");
-	Fail("Won't be printed");
-}
-
-
-void test_assert_int()
-{
-	int a=4, b=3, c=4;
-
-	// These should all pass
-	AssertEq(a,c);
-	AssertNe(a,b);
-	AssertGt(a,b);
-	AssertGe(a,b);
-	AssertGe(a,c);
-	AssertLt(b,a);
-	AssertLe(b,a);
-	AssertLe(c,a);
-	AssertIntEq(a,c);
-	AssertIntNe(a,b);
-	AssertIntGt(a,b);
-	AssertIntGe(a,b);
-	AssertIntGe(a,c);
-	AssertIntLt(b,a);
-	AssertIntLe(b,a);
-	AssertIntLe(c,a);
-	AssertEqHex(a,c);
-	AssertNeHex(a,b);
-	AssertGtHex(a,b);
-	AssertGeHex(a,b);
-	AssertGeHex(a,c);
-	AssertLtHex(b,a);
-	AssertLeHex(b,a);
-	AssertLeHex(c,a);
-}
-
-void test_assert_int_eq_fail()
-{
-	int a=4, b=3;
-	AssertEq(a,b);
-}
-void test_assert_int_ne_fail()
-{
-	int a=4, b=3;
-	AssertNe(a,b);
-}
-void test_assert_int_gt_fail()
-{
-	int a=4, b=3;
-	AssertGt(a,b);
-}
-void test_assert_int_ge_fail()
-{
-	int a=4, b=3;
-	AssertGe(a,b);
-}
-void test_assert_int_lt_fail()
-{
-	int a=4, b=3;
-	AssertLt(a,b);
-}
-void test_assert_int_le_fail()
-{
-	int a=4, b=3;
-	AssertLe(a,b);
-}
-
-void test_assert_ptr()
-{
-	int a, b;
-	int *ap = &a;
-	int *bp = &b;
-	int *cp = &a;
-	int *n = NULL;
-
-	// These should all pass
-	AssertPtr(ap);
-	AssertNull(n);
-	AssertPtrEq(ap,cp);
-	AssertPtrNe(ap,bp);
-	AssertPtrGt(ap,bp);
-	AssertPtrGe(ap,bp);
-	AssertPtrGe(ap,cp);
-	AssertPtrLt(bp,ap);
-	AssertPtrLe(bp,ap);
-	AssertPtrLe(cp,ap);
-}
-
-void test_assert_ptr_fail()
-{
-	AssertPtr(NULL);
-}
-
-void test_assert_ptr_null_fail()
-{
-	void (*p)() = &test_assert_ptr_null_fail;
-	AssertPtrNull(p);
-}
-
-void test_assert_int_hex_eq_fail()
-{
-	int a=431, b=577;
-	AssertEqHex(a,b);
-}
-
-void test_assert_float()
-{
-	float a=0.0004, b=0.0003, c=0.0004;
-
-	// These should all pass
-	AssertFloatEq(a,c);
-	AssertFloatNe(a,b);
-	AssertFloatGt(a,b);
-	AssertFloatGe(a,b);
-	AssertFloatGe(a,c);
-	AssertFloatLt(b,a);
-	AssertFloatLe(b,a);
-	AssertFloatLe(c,a);
-	AssertDblEq(a,c);
-	AssertDblNe(a,b);
-	AssertDblGt(a,b);
-	AssertDblGe(a,b);
-	AssertDblGe(a,c);
-	AssertDblLt(b,a);
-	AssertDblLe(b,a);
-	AssertDblLe(c,a);
-	AssertDoubleEq(a,c);
-	AssertDoubleNe(a,b);
-	AssertDoubleGt(a,b);
-	AssertDoubleGe(a,b);
-	AssertDoubleGe(a,c);
-	AssertDoubleLt(b,a);
-	AssertDoubleLe(b,a);
-	AssertDoubleLe(c,a);
-}
-
-
-void test_assert_float_eq_fail()
-{
-	double a=1.0/3.0, b=0.4;
-	AssertFloatEq(a,b);
-}
-
-void test_assert_strings()
-{
-	const char *a = "Bogozity";
-	const char *b = "Arclamp";
-	const char *c = "Bogozity";
-
-	// These should all pass
-	AssertStrEq(a,c);
-	AssertStrNe(a,b);
-	AssertStrGt(a,b);
-	AssertStrGe(a,b);
-	AssertStrGe(a,c);
-	AssertStrLt(b,a);
-	AssertStrLe(b,a);
-	AssertStrLe(c,a);
-}
-
-void test_assert_string_eq_fail()
-{
-	const char *a = "A";
-	const char *b = "a";
-
-	AssertStrEq(a,b);
-}
-
-
-void test_assert_string_ne_fail()
-{
-	const char *a = "A";
-	const char *b = "A";
-
-	AssertStrNe(a,b);
-}
-
-void test_assert_string_gt_fail()
-{
-	const char *a = "A";
-	const char *b = "A";
-
-	AssertStrGt(a,b);
-}
-
-zutest_proc zutest_tests[] = {
-	test_assert_int,
-	test_assert_ptr,
-	test_assert_float,
-	test_assert_strings,
-	NULL
-};
-
-zutest_proc zutest_empty_suite[] = {
-	NULL
-};
-
-zutest_proc zutest_failures[] = {
-	test_fail,
-	test_assert_int_eq_fail,
-	test_assert_ptr_fail,
-	test_assert_ptr_null_fail,
-	test_assert_int_hex_eq_fail,
-	test_assert_float_eq_fail,
-	test_assert_string_eq_fail,
-	test_assert_string_ne_fail,
-	test_assert_string_gt_fail,
-	NULL
-};
-
-zutest_suite all_zutests[] = {
-	zutest_tests,
-	zutest_empty_suite,
-	zutest_failures,
-	NULL
-};
-
-
-int main(int argc, char **argv)
-{
-	run_unit_tests(all_zutests);
-	return 0;
-}
-

Modified: trunk/zutest.c
===================================================================
--- trunk/zutest.c	2007-01-24 23:51:11 UTC (rev 125)
+++ trunk/zutest.c	2007-01-25 01:02:48 UTC (rev 126)
@@ -1,6 +1,8 @@
 /* zutest.c
  * Scott Bronson
  * 6 Mar 2006
+ *
+ * Version 0.6, 26 Apr 2006
  */
 
 #include <stdio.h>
@@ -11,23 +13,34 @@
 #include "zutest.h"
 
 
-/**
- * A single procedure is called a test.  If any of the asserts fail
- * within a test, the test itself is stopped but all other tests will
- * be run.
+/** @file zutest.c
  *
- * Most files will contain a number of tests.  These tests are organized
- * into a suite.  A test suite contains one or more tests.
+ * This file contains all of the test mechanisms provided by the
+ * Zutest unit testing framework.
  *
- * Test suites are organized into a test battery.  You don't need to
- * know this except when specifying all the suites that will be tested.
+ * A single function is called a test.  If any of the asserts fail
+ * within a test, the test itself is stopped and printed as a failure
+ * but all other tests in the current test suite, and all other test
+ * suites, will still be run.
+ *
+ * A test suite consists of a number of tests.  Typically a C file
+ * will include a test suite that lists all the tests in the file.
+ *
+ * TODO: print test results, test suites, etc as they run.
+ *    Add a quiet flag that will suppress printing unless a test fails.
+ *    quiet=0, full printing
+ *    quiet=1, test results not printed
+ *    quiet=2, suite results not printed
+ *    quiet=3, summary not printed.
  */
 
 
-static jmp_buf test_bail;
-static int tests_run;
-static int successes;
-static int failures;
+static jmp_buf test_bail;	///< If a test fails, this is where we end up.
+int zutest_assertions;		///< A goofy statistic, updated by the assertion macros
+static int tests_run;		///< The number of tests that we have run.  successes+failures==tests_run (if not, then there's a bug somewhere).
+static int successes;		///< The number of successful tests run
+static int failures;		///< The number of failed tests run.
+static jmp_buf *inversion;	///< Where to go if the assertion fails.  This is NULL except when running Zutest's internal unit tests.  See test_fail().
 
 
 void zutest_fail(const char *file, int line, const char *func, 
@@ -35,6 +48,11 @@
 {
 	va_list ap;
 
+	// If inversion is set, then an assert correctly failed.
+	if(inversion) {
+		longjmp(*inversion, 1);
+	}
+
 	fprintf(stderr, "FAIL %s at %s line %d:\n\t", func, file, line);
 	va_start(ap, msg);
 	vfprintf(stderr, msg, ap);
@@ -61,11 +79,11 @@
 }
 
 
-void run_zutest_battery(const zutest_battery battery)
+void run_zutest_suites(const zutest_suites suites)
 {
 	zutest_suite *suite;
 
-	for(suite=battery; *suite; suite++) {
+	for(suite=suites; *suite; suite++) {
 		run_zutest_suite(*suite);
 	}
 }
@@ -74,19 +92,27 @@
 void print_zutest_results()
 {
 	if(failures == 0) {
-		printf("%d tests run, %d successes.\n", successes, successes);
+		printf("All OK.  %d test%s run, %d successe%s (%d assertion%s).\n",
+				successes, (successes == 1 ? "" : "s"),
+				successes, (successes == 1 ? "" : "s"),
+				zutest_assertions, (zutest_assertions == 1 ? "" : "s"));
 		return;
 	}
 
-	printf("%d failures of %d tests run!\n", failures, tests_run);
+	printf("ERROR: %d failure%s in %d test%s run!\n",
+			failures, (failures == 1 ? "" : "s"), 
+			tests_run, (tests_run == 1 ? "" : "s"));
 }
 
 
-void run_unit_tests(const zutest_battery battery)
+/** Runs all the unit tests in all the passed-in test suites.
+ */
+
+void run_unit_tests(const zutest_suites suites)
 {
-	run_zutest_battery(battery);
+	run_zutest_suites(suites);
 	print_zutest_results();
-	exit(failures < 99 ? failures : 99);
+	exit(failures < 100 ? failures : 100);
 }
 
 
@@ -101,10 +127,283 @@
  * without doing anything.
  */
 
-void unit_test_check(int argc, char **argv, const zutest_battery battery)
+void unit_test_check(int argc, char **argv, const zutest_suites suites)
 {
 	if(argc > 1 && strcmp(argv[1],"--run-unit-tests") == 0) {
-		run_unit_tests(battery);
+		run_unit_tests(suites);
 	}
 }
 
+
+
+
+
+#if defined(ZUTEST) || defined(ZUTEST_MAIN)
+
+/* This code runs the zutest unit tests to ensure that zutest itself
+ * is working properly.
+ */
+
+
+/** This macro is used to reverse the sense of the tests. 
+ *
+ * To properly test Zutest, we need to ensure that the Assert macros
+ * handle failures too.  Therefore, we occasionally want to reverse
+ * the sense of the macro, where a failure indicates a successful test
+ * and a passing assert means that the test has failed.
+ *
+ * This macro inverts the sense of the contained assertion.
+ * test_failure(AssertEq(a,b)) causes the test to pass
+ * only when the assertion fails (i.e. when a != b).
+ */
+
+#define test_failure(test) 				\
+	do { 								\
+		jmp_buf jb; 					\
+		int val = setjmp(jb); 			\
+		if(val == 0) { 					\
+			inversion = &jb;			\
+			do { test; } while(0);		\
+			inversion = NULL;			\
+			Fail("This test should have failed: " #test);	\
+		}								\
+		inversion = NULL;				\
+	} while(0)
+
+
+
+void test_assert_int()
+{
+	int a=4, b=3, c=4, z=0, n=-1;
+
+	AssertEq(a,c);
+	AssertNe(a,b);
+	AssertGt(a,b);
+	AssertGe(a,b);
+	AssertGe(a,c);
+	AssertLt(b,a);
+	AssertLe(b,a);
+	AssertLe(c,a);
+
+	test_failure( AssertEq(a,b) );
+	test_failure( AssertNe(a,c) );
+	test_failure( AssertGt(a,c) );
+	test_failure( AssertGt(b,c) );
+	test_failure( AssertGe(b,a) );
+	test_failure( AssertLt(c,a) );
+	test_failure( AssertLt(c,b) );
+	test_failure( AssertLe(a,b) );
+
+	AssertZero(z);
+	test_failure( AssertZero(a) );
+	AssertNonzero(a);
+	test_failure( AssertNonzero(z) );
+
+	AssertPositive(a);
+	test_failure( AssertPositive(z) );
+	test_failure( AssertPositive(n) );
+
+	AssertNonPositive(n);
+	AssertNonPositive(z);
+	test_failure( AssertNonPositive(a) );
+
+	AssertNegative(n);
+	test_failure( AssertNegative(z) );
+	test_failure( AssertNegative(a) );
+
+	AssertNonNegative(a);
+	AssertNonNegative(z);
+	test_failure( AssertNonNegative(n) );
+}
+
+
+void test_assert_hex()
+{
+	int a=4, b=3, c=4, z=0, n=-1;
+
+	AssertEqHex(a,c);
+	AssertNeHex(a,b);
+	AssertGtHex(a,b);
+	AssertGeHex(a,b);
+	AssertGeHex(a,c);
+	AssertLtHex(b,a);
+	AssertLeHex(b,a);
+	AssertLeHex(c,a);
+
+	test_failure( AssertEqHex(a,b) );
+	test_failure( AssertNeHex(a,c) );
+	test_failure( AssertGtHex(a,c) );
+	test_failure( AssertGtHex(b,c) );
+	test_failure( AssertGeHex(b,a) );
+	test_failure( AssertLtHex(c,a) );
+	test_failure( AssertLtHex(c,b) );
+	test_failure( AssertLeHex(a,b) );
+
+	AssertZeroHex(z);
+	test_failure( AssertZeroHex(a) );
+	AssertNonzeroHex(a);
+	test_failure( AssertNonzeroHex(z) );
+
+	AssertPositiveHex(a);
+	test_failure( AssertPositiveHex(z) );
+	test_failure( AssertPositiveHex(n) );
+
+	AssertNonPositiveHex(n);
+	AssertNonPositiveHex(z);
+	test_failure( AssertNonPositiveHex(a) );
+
+	AssertNegativeHex(n);
+	test_failure( AssertNegativeHex(z) );
+	test_failure( AssertNegativeHex(a) );
+
+	AssertNonNegativeHex(a);
+	AssertNonNegativeHex(z);
+	test_failure( AssertNonNegativeHex(n) );
+}
+
+
+void test_assert_ptr()
+{
+	int a, b;
+	int *ap = &a;
+	int *bp = &b;
+	int *cp = &a;
+	int *n = NULL;
+
+	AssertPtr(ap);
+	AssertNull(n);
+
+	test_failure( AssertPtr(n) );
+	test_failure( AssertNull(ap) );
+
+	AssertPtrEq(ap,cp);
+	AssertPtrNe(ap,bp);
+	AssertPtrGt(ap,bp);
+	AssertPtrGe(ap,bp);
+	AssertPtrGe(ap,cp);
+	AssertPtrLt(bp,ap);
+	AssertPtrLe(bp,ap);
+	AssertPtrLe(cp,ap);
+
+	test_failure( AssertPtrEq(ap,bp) );
+	test_failure( AssertPtrNe(ap,cp) );
+	test_failure( AssertPtrGt(ap,cp) );
+	test_failure( AssertPtrGt(bp,cp) );
+	test_failure( AssertPtrGe(bp,ap) );
+	test_failure( AssertPtrLt(cp,ap) );
+	test_failure( AssertPtrLt(cp,bp) );
+	test_failure( AssertPtrLe(ap,bp) );
+}
+
+
+void test_assert_float()
+{
+	float a=0.0004, b=0.0003, c=0.0004;
+
+	AssertFloatEq(a,c);
+	AssertFloatNe(a,b);
+	AssertFloatGt(a,b);
+	AssertFloatGe(a,b);
+	AssertFloatGe(a,c);
+	AssertFloatLt(b,a);
+	AssertFloatLe(b,a);
+	AssertFloatLe(c,a);
+
+	test_failure( AssertFloatEq(a,b) );
+	test_failure( AssertFloatNe(a,c) );
+	test_failure( AssertFloatGt(a,c) );
+	test_failure( AssertFloatGt(b,c) );
+	test_failure( AssertFloatGe(b,a) );
+	test_failure( AssertFloatLt(c,a) );
+	test_failure( AssertFloatLt(c,b) );
+	test_failure( AssertFloatLe(a,b) );
+
+	AssertDoubleEq(a,c);
+	AssertDoubleNe(a,b);
+	AssertDoubleGt(a,b);
+	AssertDoubleGe(a,b);
+	AssertDoubleGe(a,c);
+	AssertDoubleLt(b,a);
+	AssertDoubleLe(b,a);
+	AssertDoubleLe(c,a);
+
+	test_failure( AssertDoubleEq(a,b) );
+	test_failure( AssertDoubleNe(a,c) );
+	test_failure( AssertDoubleGt(a,c) );
+	test_failure( AssertDoubleGt(b,c) );
+	test_failure( AssertDoubleGe(b,a) );
+	test_failure( AssertDoubleLt(c,a) );
+	test_failure( AssertDoubleLt(c,b) );
+	test_failure( AssertDoubleLe(a,b) );
+}
+
+
+void test_assert_strings()
+{
+	const char *a = "Bogozity";
+	const char *b = "Arclamp";
+	const char *c = "Bogozity";
+	const char *e = "";
+	const char *n = NULL;
+
+	AssertStrEq(a,c);
+	AssertStrNe(a,b);
+	AssertStrGt(a,b);
+	AssertStrGe(a,b);
+	AssertStrGe(a,c);
+	AssertStrLt(b,a);
+	AssertStrLe(b,a);
+	AssertStrLe(c,a);
+
+	test_failure( AssertStrEq(a,b) );
+	test_failure( AssertStrNe(a,c) );
+	test_failure( AssertStrGt(a,c) );
+	test_failure( AssertStrGt(b,c) );
+	test_failure( AssertStrGe(b,a) );
+	test_failure( AssertStrLt(c,a) );
+	test_failure( AssertStrLt(c,b) );
+	test_failure( AssertStrLe(a,b) );
+
+	AssertStrEmpty(e);
+	test_failure( AssertStrEmpty(a) );
+	test_failure( AssertStrEmpty(n) );
+
+	AssertStrNonEmpty(a);
+	test_failure( AssertStrNonEmpty(e) );
+	test_failure( AssertStrNonEmpty(n) );
+}
+
+
+zutest_proc zutest_tests[] = {
+	test_assert_int,
+	test_assert_hex,
+	test_assert_ptr,
+	test_assert_float,
+	test_assert_strings,
+	NULL
+};
+
+
+// Ensure that zutest doesn't crash if handed an empty suite.
+zutest_proc zutest_empty_suite[] = {
+	NULL
+};
+
+
+zutest_suite all_zutests[] = {
+	zutest_empty_suite,
+	zutest_tests,
+	NULL
+};
+
+
+#ifdef ZUTEST_MAIN
+int main(int argc, char **argv)
+{
+	run_unit_tests(all_zutests);
+	return 0;
+}
+#endif
+
+#endif
+

Modified: trunk/zutest.h
===================================================================
--- trunk/zutest.h	2007-01-24 23:51:11 UTC (rev 125)
+++ trunk/zutest.h	2007-01-25 01:02:48 UTC (rev 126)
@@ -2,118 +2,270 @@
  * Scott Bronson
  * 6 Mar 2006
  *
- * This is a ground-up rewrite of Asim Jalis's "CuTest" library.
+ * TODO: make tests self-documenting.  The test name is the same as the
+ * function name, but they should also have a short and long description.
+ * TODO: make zutest suites able to be arranged in a hierarchy.
+ *
+ * Version 0.62, 22 Jan 2007
+ * Version 0.61, 30 Apr 2006
+ */
+
+
+/* @file zutest.h
+ *
+ * This file contains the declarations and all the Assert macros
+ * required to use Zutest in your own applications.
+ *
+ * Zutest is a ground-up rewrite of Asim Jalis's "CuTest" library.
  * It is released under the MIT License.
+ *
+ * To compile Zutest to run its own unit tests, do this:
+ * 
+ * <pre>
+ * 	$ cc -DZUTEST_MAIN zutest.c -o zutest
+ * 	$ ./zutest
+ * 	4 tests run, 4 successes (132 assertions).
+ * </pre>
+ *
+ * If your non-gcc compiler complains about a missing __func__ macro,
+ * add -D__func__='"test"' to the compiler's command line.
+ *
+ * See ::zutest_tests for instructions on how to add zutest's
+ * built-in unit tests to your application's test suite.
  */
 
 
-// If your compiler doesn't provide __func__, compile with -D__func__='"test"'
+#ifndef ZUTEST_H
+#define ZUTEST_H
+
+// Note that Fail doesn't increment zutest_assertions (the number of assertions
+// that have been made) because it doesn't assert anything.  It only fails.
+// If you call fail, you might want to increment zutest_assertions
+// manually if you care about this number.  Normally you won't care.
 #define Fail(...) zutest_fail(__FILE__, __LINE__, __func__, __VA_ARGS__)
+
 // If the expression returns false, it is printed in the failure message.
-#define Assert(x) do { if(!(x)) { Fail(#x); } } while(0)
+#define Assert(x) do { zutest_assertions++; \
+		if(!(x)) { Fail(#x); } } while(0)
+
 // If the expression returns false, the given format string is printed.
-#define AssertFmt(x,...) do { if(!(x)) { Fail(__VA_ARGS__); } } while(0)
-// On failure the expression is printed followed by the format string.
-#define AssertExp(ex,...) AssertFmt(ex,#ex __VA_ARGS__)
+// This is the same as Assert, just with much more helpful error messages.
+// For instance: AssertFmt(isdigit(x), "isdigit but x=='%c'", x);
+#define AssertFmt(x,...) do { zutest_assertions++; \
+		if(!(x)) { Fail(__VA_ARGS__); } } while(0)
 
-#define AssertExpType(x,y,op,type,fmt) \
-	AssertExp(x op y," but "#x"=="fmt" and "#y"=="fmt"!",(type)x,(type)y)
-
-// get ready...
-// These only work with integer and pointer values!
+// integers, longs, chars...
 #define AssertEq(x,y) AssertOp(x,y,==)
 #define AssertNe(x,y) AssertOp(x,y,!=)
 #define AssertGt(x,y) AssertOp(x,y,>)
 #define AssertGe(x,y) AssertOp(x,y,>=)
 #define AssertLt(x,y) AssertOp(x,y,<)
 #define AssertLe(x,y) AssertOp(x,y,<=)
-#define AssertPositive(x) AssertOp(x,0,>)
-#define AssertNegative(x) AssertOp(x,0,<)
-#define AssertNonNegative(x) AssertOp(x,0,>=)
-#define AssertIntEq(x,y) AssertOp(x,y,==)
-#define AssertIntNe(x,y) AssertOp(x,y,!=)
-#define AssertIntGt(x,y) AssertOp(x,y,>)
-#define AssertIntGe(x,y) AssertOp(x,y,>=)
-#define AssertIntLt(x,y) AssertOp(x,y,<)
-#define AssertIntLe(x,y) AssertOp(x,y,<=)
-#define AssertOp(x,y,op) AssertExpType(x,y,op,long,"%ld")
-// Same as above but the values are printed in hex rather than decimal.
+
+#define AssertZero(x) AssertOpToZero(x,==)
+#define AssertNonzero(x) AssertOpToZero(x,!=)
+#define AssertNonZero(x) AssertNonzero(x)
+#define AssertPositive(x) AssertOpToZero(x,>);
+#define AssertNegative(x) AssertOpToZero(x,<);
+#define AssertNonNegative(x) AssertOpToZero(x,>=);
+#define AssertNonPositive(x) AssertOpToZero(x,<=);
+
+// Also integers but failure values are printed in hex rather than decimal.
 #define AssertEqHex(x,y) AssertHexOp(x,y,==)
 #define AssertNeHex(x,y) AssertHexOp(x,y,!=)
 #define AssertGtHex(x,y) AssertHexOp(x,y,>)
 #define AssertGeHex(x,y) AssertHexOp(x,y,>=)
 #define AssertLtHex(x,y) AssertHexOp(x,y,<)
 #define AssertLeHex(x,y) AssertHexOp(x,y,<=)
-// Same as above but all 8 digits are printed
-// If you give me a 64 bit computer, I will give you 16 digits!
-#define AssertHexOp(x,y,op) AssertExpType(x,y,op,long,"0x%lX")
+
+#define AssertZeroHex(x) AssertHexOpToZero(x,==)
+#define AssertNonzeroHex(x) AssertHexOpToZero(x,!=)
+#define AssertNonZeroHex(x) AssertNonzeroHex(x)
+#define AssertPositiveHex(x) AssertHexOpToZero(x,>);
+#define AssertNegativeHex(x) AssertHexOpToZero(x,<);
+#define AssertNonNegativeHex(x) AssertHexOpToZero(x,>=);
+#define AssertNonPositiveHex(x) AssertHexOpToZero(x,<=);
+
+// Pointers...
 #define AssertPtr(p)  AssertFmt(p != NULL, \
-		#p" != NULL but "#p"==0x%08lX!", (unsigned long)p)
+		#p" != NULL but "#p"==0x%lX!", (unsigned long)p)
 #define AssertNull(p) AssertFmt(p == NULL, \
-		#p" == NULL but "#p"==0x%08lX!", (unsigned long)p)
+		#p" == NULL but "#p"==0x%lX!", (unsigned long)p)
+#define AssertNonNull(p) AssertPtr(p)
+
 #define AssertPtrNull(p) AssertNull(p)
+#define AssertPtrNonNull(p) AssertNonNull(p)
 #define AssertPtrEq(x,y) AssertPtrOp(x,y,==)
 #define AssertPtrNe(x,y) AssertPtrOp(x,y,!=)
 #define AssertPtrGt(x,y) AssertPtrOp(x,y,>)
 #define AssertPtrGe(x,y) AssertPtrOp(x,y,>=)
 #define AssertPtrLt(x,y) AssertPtrOp(x,y,<)
 #define AssertPtrLe(x,y) AssertPtrOp(x,y,<=)
-#define AssertPtrOp(x,y,op) AssertExpType(x,y,op,unsigned long,"0x%lX")
+
 // These work with floats and doubles
+// (everything is handled internally as double)
 #define AssertFloatEq(x,y) AssertFloatOp(x,y,==)
 #define AssertFloatNe(x,y) AssertFloatOp(x,y,!=)
 #define AssertFloatGt(x,y) AssertFloatOp(x,y,>)
 #define AssertFloatGe(x,y) AssertFloatOp(x,y,>=)
 #define AssertFloatLt(x,y) AssertFloatOp(x,y,<)
 #define AssertFloatLe(x,y) AssertFloatOp(x,y,<=)
-// Dbl is implemented the same as Float internally.
-// We just provide a Dbl and Double names so that the programmer can
-// use whatever name she prefers and the macro can exactly equal the type.
-#define AssertDblEq(x,y) AssertFloatOp(x,y,==)
-#define AssertDblNe(x,y) AssertFloatOp(x,y,!=)
-#define AssertDblGt(x,y) AssertFloatOp(x,y,>)
-#define AssertDblGe(x,y) AssertFloatOp(x,y,>=)
-#define AssertDblLt(x,y) AssertFloatOp(x,y,<)
-#define AssertDblLe(x,y) AssertFloatOp(x,y,<=)
+// supply Doubles so people don't worry about precision when they see Float
 #define AssertDoubleEq(x,y) AssertFloatOp(x,y,==)
 #define AssertDoubleNe(x,y) AssertFloatOp(x,y,!=)
 #define AssertDoubleGt(x,y) AssertFloatOp(x,y,>)
 #define AssertDoubleGe(x,y) AssertFloatOp(x,y,>=)
 #define AssertDoubleLt(x,y) AssertFloatOp(x,y,<)
 #define AssertDoubleLe(x,y) AssertFloatOp(x,y,<=)
-#define AssertFloatOp(x,y,op) AssertExpType(x,y,op,double,"%lf")
-// These work with strings
+
+// Strings (uses strcmp)...
 #define AssertStrEq(x,y) AssertStrOp(x,y,EQ,==)
 #define AssertStrNe(x,y) AssertStrOp(x,y,NE,!=)
 #define AssertStrGt(x,y) AssertStrOp(x,y,GT,>)
 #define AssertStrGe(x,y) AssertStrOp(x,y,GE,>=)
 #define AssertStrLt(x,y) AssertStrOp(x,y,LT,<)
 #define AssertStrLe(x,y) AssertStrOp(x,y,LE,<=)
+
+// ensures a string is non-null but zero-length
+#define AssertStrEmpty(p) do { zutest_assertions++; \
+		if(!(p)) { Fail(#p" is empty but "#p" is NULL!"); } \
+		if((p)[0]) { Fail(#p" is empty but "#p" is: %s",p); } \
+	} while(0)
+// ensures a string is non-null and non-zero-length
+#define AssertStrNonEmpty(p) do { zutest_assertions++; \
+		if(!(p)) { Fail(#p" is nonempty but "#p" is NULL!"); } \
+		if(!(p)[0]) { Fail(#p" is nonempty but "#p"[0] is 0"); } \
+	} while(0)
+
+
+
+//
+// helper macros, not intended to be called directly.
+//
+
+// On failure the expression is printed followed by the format string.
+#define AssertExp(ex,...) AssertFmt(ex,#ex __VA_ARGS__)
+// Like AssertExp but enforces a type while performing the comparison.
+#define AssertExpType(x,y,op,type,fmt) \
+	AssertExp((type)x op (type)y," failed because "#x"=="fmt" and "#y"=="fmt"!", (type)x,(type)y)
+// It's weird hearing "x==0 failed because x==1 and 0==0" so we'll
+// special-case checking against 0: x==0 failed because x==1).
+#define AssertExpToZero(x,op,type,fmt) \
+	AssertExp((type)x op 0," failed because "#x"=="fmt"!", (type)x)
+
+#define AssertOp(x,y,op) AssertExpType(x,y,op,long,"%ld")
+#define AssertHexOp(x,y,op) AssertExpType(x,y,op,long,"0x%lX")
+#define AssertOpToZero(x,op) AssertExpToZero(x,op,long,"%ld")
+#define AssertHexOpToZero(x,op) AssertExpToZero(x,op,long,"0x%lX")
+#define AssertPtrOp(x,y,op) AssertExpType(x,y,op,unsigned long,"0x%lX")
+#define AssertFloatOp(x,y,op) AssertExpType(x,y,op,double,"%lf")
 #define AssertStrOp(x,y,opn,op) AssertFmt(strcmp(x,y) op 0, \
 	#x" "#opn" "#y" but "#x" is \"%s\" and "#y" is \"%s\"!",x,y)
 
-// if none of those macros above fit your fancy, call fail directly.
-void zutest_fail(const char *file, int line, const char *func,
-		const char *msg, ...);
 
 
+/** Keeps track of how many assertions have been made.
+ * This needs to be updated manually each time an assertion
+ * is made.  The Zutest built-in assertion macros all
+ * update this variable properly.
+ */
 
+extern int zutest_assertions;
+
+
+/** A single test
+ *
+ * This routine is called to run the test.  If it returns, the test
+ * succeeds.  If zutest_fail() is called (either directly or indirectly
+ * via an Assert macro), then the test fails.
+ */
 typedef void (*zutest_proc)();
+
+
+/** A suite of tests
+ *
+ * A zutest_suite is simply a list of tests.  Generally, each .c file
+ * in your project will include a test suite that ensures all the tests
+ * contained in the .c file are run.  A suite is just a NULL-terminated
+ * list of tests.
+ */
 typedef zutest_proc *zutest_suite;
-typedef zutest_suite *zutest_battery;
 
+
+/** A suite of test suites
+ *
+ * Zutests runs through each test suite in your project, running all the
+ * tests in each suite.  A suite of suites is just a NULL-terminated list
+ * of suites.  This is the topmost data structure used by zutest.
+ * TODO: make it so zutest chan handle an arbitrary hierarchy of suites.
+ * That way this data structure can go away.
+ */
+typedef zutest_suite *zutest_suites;
+
+
+/** Fails the current test.
+ *
+ * This function may only be called from within a ::zutest_proc.
+ *
+ * If none of the built-in Assert macros fit your fancy, you can do the
+ * check on your own and call zutest_fail in the event that it fails.
+ * 
+ * Example:
+ * 
+ * <pre>
+ * if(my_error) {
+ *    zutest_fail(__FILE__, __LINE__, __func__, "Error Message %d", 1);
+ * }
+ * </pre>
+ *
+ * But, really, it's easier just to call the Fail() macro.
+ */
+
+void zutest_fail(const char *file, int line, const char *func,
+		const char *msg, ...);
+
+
+/** Runs all the tests in a suite. */
 void run_zutest_suite(const zutest_suite suite);
-void run_zutest_battery(const zutest_battery battery);
+/** Runs all the tests in all the suites passed in. */
+void run_zutest_suites(const zutest_suites suites);
+
 void print_zutest_results();
 
-// Call this on the very first line of your application.  If the user
-// ran your program with the first arg of "--run-unit-tests", this will
-// run the tests and exit.  Otherwise your program will run as normal.
-// If you would rather create a dedicated executable, just call
-// run_zutest_battery() directly.
-void unit_test_check(int argc, char **argv, const zutest_battery battery);
 
-// This runs all the unit tests supplied and then exits.  Use this
-// if you want to handle the arguments yourself.
-void run_unit_tests(const zutest_battery battery);
+/** 
+ *
+ * Call this on the very first line of your application.  If the user
+ * ran your program with the first arg of "--run-unit-tests", this will
+ * run the tests and exit.  Otherwise your program will run as normal.
+ * If you would rather create a dedicated executable, just call
+ * run_zutest_suites() directly.
+ */
 
+void unit_test_check(int argc, char **argv, const zutest_suites suites);
+
+/**
+ *
+ * This runs all the unit tests supplied and then exits.  Use this
+ * if you want to handle the arguments yourself.
+ */
+
+void run_unit_tests(const zutest_suites suites);
+
+
+/** Zutest's built-in test suite.
+ *
+ * This allows you to add the Zutest unit test suite to your application's
+ * test suites.  This way, you can ensure that Zutest's unit tests pass
+ * before running your application's.  This is for the especially pedantic. :)
+ *
+ * Unfortunately, there is one test that cannot be run if you do this:
+ * ensuring that zutest properly handles empty test suites.
+ * Other than this one test, adding zutest_tests
+ * to your application's test suite is equivalent to causing zutest to
+ * compile and run its unit tests as described in zutest.h.
+ */
+
+extern zutest_proc zutest_tests[];
+
+#endif





From tmtest-commits at lists.berlios.de  Thu Jan 25 02:07:36 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Thu, 25 Jan 2007 01:07:36 -0000
Subject: [Tmtest-commits] [127] trunk: Run the zutest unit tests before we
	run our own.
Message-ID: <200701250106.l0P1690H023054@sheep.berlios.de>

Revision: 127
Author:   bronson
Date:     2007-01-25 02:06:08 +0100 (Thu, 25 Jan 2007)

Log Message:
-----------
Run the zutest unit tests before we run our own.

Modified Paths:
--------------
    trunk/Makefile
    trunk/test/02-running/03-Unit-Tests.test
    trunk/units.c
Modified: trunk/Makefile
===================================================================
--- trunk/Makefile	2007-01-25 01:02:48 UTC (rev 126)
+++ trunk/Makefile	2007-01-25 01:06:08 UTC (rev 127)
@@ -48,7 +48,7 @@
 
 
 tmtest: $(CSRC) $(CHDR) $(INTERMED)
-	$(CC) $(COPTS) $(CSRC) -o tmtest -DVERSION="$(VERSION)"
+	$(CC) $(COPTS) $(CSRC) -o tmtest -DVERSION="$(VERSION)" -DZUTEST
 
 template.c: template.sh cstrfy
 	./cstrfy -n exec_template < template.sh > template.c

Modified: trunk/test/02-running/03-Unit-Tests.test
===================================================================
--- trunk/test/02-running/03-Unit-Tests.test	2007-01-25 01:02:48 UTC (rev 126)
+++ trunk/test/02-running/03-Unit-Tests.test	2007-01-25 01:06:08 UTC (rev 127)
@@ -3,4 +3,4 @@
 $tmtest --run-unit-tests
 
 STDOUT:
-All OK.  6 tests run, 6 successes (57 assertions).
+All OK.  11 tests run, 11 successes (195 assertions).

Modified: trunk/units.c
===================================================================
--- trunk/units.c	2007-01-25 01:02:48 UTC (rev 126)
+++ trunk/units.c	2007-01-25 01:06:08 UTC (rev 127)
@@ -4,6 +4,7 @@
 
 
 zutest_suite all_unit_tests[] = {
+	zutest_tests,	// run a self-check on the unit test library
 	compare_tests,
 	NULL
 };





From tmtest-commits at lists.berlios.de  Thu Jan 25 03:32:15 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Thu, 25 Jan 2007 02:32:15 -0000
Subject: [Tmtest-commits] [128] trunk: Add the ability to view zutest
	failures (zutest -f).
Message-ID: <200701250230.l0P2UraP001537@sheep.berlios.de>

Revision: 128
Author:   bronson
Date:     2007-01-25 03:30:51 +0100 (Thu, 25 Jan 2007)

Log Message:
-----------
Add the ability to view zutest failures (zutest -f).
Clean up a number of the failures.

Modified Paths:
--------------
    trunk/TODO
    trunk/zutest.c
    trunk/zutest.h
Modified: trunk/TODO
===================================================================
--- trunk/TODO	2007-01-25 01:06:08 UTC (rev 127)
+++ trunk/TODO	2007-01-25 02:30:51 UTC (rev 128)
@@ -3,6 +3,7 @@
   in the testfile with each line prefixed by :.  Then STDOUT.  No need
   for this delimiter craziness.
   YES, stderr ALWAYS comes first in the testfile, followed by stdout.
+  Should buffer stderr to memory, then compare stdout on the fly
 - MKFILE should just create the file that the user names.
 - MKTMPFILE should create a filename and store that in the user's variable.
 - Write unit tests for path normalization.  Maybe for CURPATH too.

Modified: trunk/zutest.c
===================================================================
--- trunk/zutest.c	2007-01-25 01:06:08 UTC (rev 127)
+++ trunk/zutest.c	2007-01-25 02:30:51 UTC (rev 128)
@@ -35,12 +35,13 @@
  */
 
 
-static jmp_buf test_bail;	///< If a test fails, this is where we end up.
-int zutest_assertions;		///< A goofy statistic, updated by the assertion macros
-static int tests_run;		///< The number of tests that we have run.  successes+failures==tests_run (if not, then there's a bug somewhere).
-static int successes;		///< The number of successful tests run
-static int failures;		///< The number of failed tests run.
-static jmp_buf *inversion;	///< Where to go if the assertion fails.  This is NULL except when running Zutest's internal unit tests.  See test_fail().
+int zutest_assertions = 0;		///< A goofy statistic, updated by the assertion macros
+static int tests_run = 0;		///< The number of tests that we have run.  successes+failures==tests_run (if not, then there's a bug somewhere).
+static int successes = 0;		///< The number of successful tests run
+static int failures = 0;		///< The number of failed tests run.
+static jmp_buf test_bail;		///< If an assertion fails, and we're not inverted, this is where we end up.
+static jmp_buf *inversion;		///< If an assertion fails, and we're inverted, this is where we end up.  This is NULL except when running Zutest's internal unit tests.  See test_fail().
+static int show_failures = 0; 	///< Set this to 1 to print the failures.  This allows you to view the output of each failure to ensure it looks OK.
 
 
 void zutest_fail(const char *file, int line, const char *func, 
@@ -48,17 +49,19 @@
 {
 	va_list ap;
 
+	if(!inversion || show_failures) {
+		fprintf(stderr, "FAIL %s at %s line %d:\n\t", func, file, line);
+		va_start(ap, msg);
+		vfprintf(stderr, msg, ap);
+		va_end(ap);
+		fputc('\n', stderr);
+	}
+
 	// If inversion is set, then an assert correctly failed.
 	if(inversion) {
 		longjmp(*inversion, 1);
 	}
 
-	fprintf(stderr, "FAIL %s at %s line %d:\n\t", func, file, line);
-	va_start(ap, msg);
-	vfprintf(stderr, msg, ap);
-	va_end(ap);
-	fputc('\n', stderr);
-
 	longjmp(test_bail, 1);
 }
 
@@ -400,6 +403,11 @@
 #ifdef ZUTEST_MAIN
 int main(int argc, char **argv)
 {
+	if(argc > 1) {
+		// "zutest -f" prints all the failures in the zutest unit tests.
+		// This allows you to check the output of each macro.
+		show_failures = 1;
+	}
 	run_unit_tests(all_zutests);
 	return 0;
 }

Modified: trunk/zutest.h
===================================================================
--- trunk/zutest.h	2007-01-25 01:06:08 UTC (rev 127)
+++ trunk/zutest.h	2007-01-25 02:30:51 UTC (rev 128)
@@ -119,22 +119,22 @@
 #define AssertDoubleLe(x,y) AssertFloatOp(x,y,<=)
 
 // Strings (uses strcmp)...
-#define AssertStrEq(x,y) AssertStrOp(x,y,EQ,==)
-#define AssertStrNe(x,y) AssertStrOp(x,y,NE,!=)
-#define AssertStrGt(x,y) AssertStrOp(x,y,GT,>)
-#define AssertStrGe(x,y) AssertStrOp(x,y,GE,>=)
-#define AssertStrLt(x,y) AssertStrOp(x,y,LT,<)
-#define AssertStrLe(x,y) AssertStrOp(x,y,LE,<=)
+#define AssertStrEq(x,y) AssertStrOp(x,y,eq,==)
+#define AssertStrNe(x,y) AssertStrOp(x,y,ne,!=)
+#define AssertStrGt(x,y) AssertStrOp(x,y,gt,>)
+#define AssertStrGe(x,y) AssertStrOp(x,y,ge,>=)
+#define AssertStrLt(x,y) AssertStrOp(x,y,lt,<)
+#define AssertStrLe(x,y) AssertStrOp(x,y,le,<=)
 
 // ensures a string is non-null but zero-length
 #define AssertStrEmpty(p) do { zutest_assertions++; \
-		if(!(p)) { Fail(#p" is empty but "#p" is NULL!"); } \
-		if((p)[0]) { Fail(#p" is empty but "#p" is: %s",p); } \
+		if(!(p)) { Fail(#p" should be empty but it is NULL!"); } \
+		if((p)[0]) { Fail(#p" should be empty but it is: %s",p); } \
 	} while(0)
 // ensures a string is non-null and non-zero-length
 #define AssertStrNonEmpty(p) do { zutest_assertions++; \
-		if(!(p)) { Fail(#p" is nonempty but "#p" is NULL!"); } \
-		if(!(p)[0]) { Fail(#p" is nonempty but "#p"[0] is 0"); } \
+		if(!(p)) { Fail(#p" should be nonempty but it is NULL!"); } \
+		if(!(p)[0]) { Fail(#p" should be nonempty but "#p"[0] is 0"); } \
 	} while(0)
 
 
@@ -143,15 +143,13 @@
 // helper macros, not intended to be called directly.
 //
 
-// On failure the expression is printed followed by the format string.
-#define AssertExp(ex,...) AssertFmt(ex,#ex __VA_ARGS__)
-// Like AssertExp but enforces a type while performing the comparison.
 #define AssertExpType(x,y,op,type,fmt) \
-	AssertExp((type)x op (type)y," failed because "#x"=="fmt" and "#y"=="fmt"!", (type)x,(type)y)
-// It's weird hearing "x==0 failed because x==1 and 0==0" so we'll
+	AssertFmt((type)x op (type)y, #x" "#op" "#y" failed because " \
+	#x"=="fmt" and "#y"=="fmt"!", (type)x,(type)y)
+// The failure "x==0 failed because x==1 and 0==0" s too wordy so we'll
 // special-case checking against 0: x==0 failed because x==1).
 #define AssertExpToZero(x,op,type,fmt) \
-	AssertExp((type)x op 0," failed because "#x"=="fmt"!", (type)x)
+	AssertFmt((type)x op 0,#x" "#op" 0 failed because "#x"=="fmt"!", (type)x)
 
 #define AssertOp(x,y,op) AssertExpType(x,y,op,long,"%ld")
 #define AssertHexOp(x,y,op) AssertExpType(x,y,op,long,"0x%lX")





From tmtest-commits at lists.berlios.de  Thu Jan 25 04:16:16 2007
From: tmtest-commits at lists.berlios.de (tmtest-commits at lists.berlios.de)
Date: Thu, 25 Jan 2007 03:16:16 -0000
Subject: [Tmtest-commits] [129] trunk: Drastically improve the output from
	failed unit tests.
Message-ID: <200701250314.l0P3EpHP003677@sheep.berlios.de>

Revision: 129
Author:   bronson
Date:     2007-01-25 04:14:50 +0100 (Thu, 25 Jan 2007)

Log Message:
-----------
Drastically improve the output from failed unit tests.

Modified Paths:
--------------
    trunk/main.c
    trunk/zutest.c
    trunk/zutest.h

Added Paths:
-----------
    trunk/test/02-running/03-Unit-Fails.test
Modified: trunk/main.c
===================================================================
--- trunk/main.c	2007-01-25 02:30:51 UTC (rev 128)
+++ trunk/main.c	2007-01-25 03:14:50 UTC (rev 129)
@@ -1059,6 +1059,7 @@
 		{"output", 0, 0, 'o'},
 		{"quiet", 0, 0, 'q'},
 		{"run-unit-tests", 0, 0, 'U'},
+		{"show-unit-fails", 0, 0, 257},
 		{"version", 0, 0, 'V'},
 		{0, 0, 0, 0},
 	};
@@ -1108,6 +1109,10 @@
 				run_unit_tests(all_unit_tests);
 				exit(0);
 
+			case 257:
+				run_unit_tests_showing_failures(all_unit_tests);
+				exit(0);
+
 			case 'V':
 				printf("tmtest version %s\n", stringify(VERSION));
 				exit(0);

Added: trunk/test/02-running/03-Unit-Fails.test
===================================================================
--- trunk/test/02-running/03-Unit-Fails.test	2007-01-25 02:30:51 UTC (rev 128)
+++ trunk/test/02-running/03-Unit-Fails.test	2007-01-25 03:14:50 UTC (rev 129)
@@ -0,0 +1,86 @@
+# This shows all the failure messages tested by zutest.
+# This tests zutest, not tmtest.  Oh well, it's worth keeping an
+# eye on our unit tester.
+
+# This won't work until we're fully event-based.
+# exec 2> >(sed s/0x[A-Z0-9][A-Z0-9]*/0xHEXDIGIT/g 1>&2)
+#  ... run commands ...
+# exec 2>&-
+
+# This doesn't work either
+# $tmtest --show-unit-fails > >(sed s/0x[A-Z0-9][A-Z0-9]*/0xHEXDIGIT/g) 2>&1
+
+$tmtest --show-unit-fails 2>&1 | sed s/0x[A-Z0-9][A-Z0-9]*/0xHEXDIGIT/g
+
+STDOUT:
+zutest.c:197: In test_assert_int, assert a == b failed. a==4 and b==3!
+zutest.c:198: In test_assert_int, assert a != c failed. a==4 and c==4!
+zutest.c:199: In test_assert_int, assert a > c failed. a==4 and c==4!
+zutest.c:200: In test_assert_int, assert b > c failed. b==3 and c==4!
+zutest.c:201: In test_assert_int, assert b >= a failed. b==3 and a==4!
+zutest.c:202: In test_assert_int, assert c < a failed. c==4 and a==4!
+zutest.c:203: In test_assert_int, assert c < b failed. c==4 and b==3!
+zutest.c:204: In test_assert_int, assert a <= b failed. a==4 and b==3!
+zutest.c:207: In test_assert_int, assert a == 0 failed. a==4!
+zutest.c:209: In test_assert_int, assert z != 0 failed. z==0!
+zutest.c:212: In test_assert_int, assert z > 0 failed. z==0!
+zutest.c:213: In test_assert_int, assert n > 0 failed. n==-1!
+zutest.c:217: In test_assert_int, assert a <= 0 failed. a==4!
+zutest.c:220: In test_assert_int, assert z < 0 failed. z==0!
+zutest.c:221: In test_assert_int, assert a < 0 failed. a==4!
+zutest.c:225: In test_assert_int, assert n >= 0 failed. n==-1!
+zutest.c:242: In test_assert_hex, assert a == b failed. a==0xHEXDIGIT and b==0xHEXDIGIT!
+zutest.c:243: In test_assert_hex, assert a != c failed. a==0xHEXDIGIT and c==0xHEXDIGIT!
+zutest.c:244: In test_assert_hex, assert a > c failed. a==0xHEXDIGIT and c==0xHEXDIGIT!
+zutest.c:245: In test_assert_hex, assert b > c failed. b==0xHEXDIGIT and c==0xHEXDIGIT!
+zutest.c:246: In test_assert_hex, assert b >= a failed. b==0xHEXDIGIT and a==0xHEXDIGIT!
+zutest.c:247: In test_assert_hex, assert c < a failed. c==0xHEXDIGIT and a==0xHEXDIGIT!
+zutest.c:248: In test_assert_hex, assert c < b failed. c==0xHEXDIGIT and b==0xHEXDIGIT!
+zutest.c:249: In test_assert_hex, assert a <= b failed. a==0xHEXDIGIT and b==0xHEXDIGIT!
+zutest.c:252: In test_assert_hex, assert a == 0 failed. a==0xHEXDIGIT!
+zutest.c:254: In test_assert_hex, assert z != 0 failed. z==0xHEXDIGIT!
+zutest.c:257: In test_assert_hex, assert z > 0 failed. z==0xHEXDIGIT!
+zutest.c:258: In test_assert_hex, assert n > 0 failed. n==0xHEXDIGIT!
+zutest.c:262: In test_assert_hex, assert a <= 0 failed. a==0xHEXDIGIT!
+zutest.c:265: In test_assert_hex, assert z < 0 failed. z==0xHEXDIGIT!
+zutest.c:266: In test_assert_hex, assert a < 0 failed. a==0xHEXDIGIT!
+zutest.c:270: In test_assert_hex, assert n >= 0 failed. n==0xHEXDIGIT!
+zutest.c:285: In test_assert_ptr, assert n != NULL failed. n==0xHEXDIGIT!
+zutest.c:286: In test_assert_ptr, assert ap == NULL failed. ap==0xHEXDIGIT!
+zutest.c:297: In test_assert_ptr, assert ap == bp failed. ap==0xHEXDIGIT and bp==0xHEXDIGIT!
+zutest.c:298: In test_assert_ptr, assert ap != cp failed. ap==0xHEXDIGIT and cp==0xHEXDIGIT!
+zutest.c:299: In test_assert_ptr, assert ap > cp failed. ap==0xHEXDIGIT and cp==0xHEXDIGIT!
+zutest.c:300: In test_assert_ptr, assert bp > cp failed. bp==0xHEXDIGIT and cp==0xHEXDIGIT!
+zutest.c:301: In test_assert_ptr, assert bp >= ap failed. bp==0xHEXDIGIT and ap==0xHEXDIGIT!
+zutest.c:302: In test_assert_ptr, assert cp < ap failed. cp==0xHEXDIGIT and ap==0xHEXDIGIT!
+zutest.c:303: In test_assert_ptr, assert cp < bp failed. cp==0xHEXDIGIT and bp==0xHEXDIGIT!
+zutest.c:304: In test_assert_ptr, assert ap <= bp failed. ap==0xHEXDIGIT and bp==0xHEXDIGIT!
+zutest.c:321: In test_assert_float, assert a == b failed. a==0.000400 and b==0.000300!
+zutest.c:322: In test_assert_float, assert a != c failed. a==0.000400 and c==0.000400!
+zutest.c:323: In test_assert_float, assert a > c failed. a==0.000400 and c==0.000400!
+zutest.c:324: In test_assert_float, assert b > c failed. b==0.000300 and c==0.000400!
+zutest.c:325: In test_assert_float, assert b >= a failed. b==0.000300 and a==0.000400!
+zutest.c:326: In test_assert_float, assert c < a failed. c==0.000400 and a==0.000400!
+zutest.c:327: In test_assert_float, assert c < b failed. c==0.000400 and b==0.000300!
+zutest.c:328: In test_assert_float, assert a <= b failed. a==0.000400 and b==0.000300!
+zutest.c:339: In test_assert_float, assert a == b failed. a==0.000400 and b==0.000300!
+zutest.c:340: In test_assert_float, assert a != c failed. a==0.000400 and c==0.000400!
+zutest.c:341: In test_assert_float, assert a > c failed. a==0.000400 and c==0.000400!
+zutest.c:342: In test_assert_float, assert b > c failed. b==0.000300 and c==0.000400!
+zutest.c:343: In test_assert_float, assert b >= a failed. b==0.000300 and a==0.000400!
+zutest.c:344: In test_assert_float, assert c < a failed. c==0.000400 and a==0.000400!
+zutest.c:345: In test_assert_float, assert c < b failed. c==0.000400 and b==0.000300!
+zutest.c:346: In test_assert_float, assert a <= b failed. a==0.000400 and b==0.000300!
+zutest.c:367: In test_assert_strings, assert a eq b failed. a is "Bogozity" and b is "Arclamp"!
+zutest.c:368: In test_assert_strings, assert a ne c failed. a is "Bogozity" and c is "Bogozity"!
+zutest.c:369: In test_assert_strings, assert a gt c failed. a is "Bogozity" and c is "Bogozity"!
+zutest.c:370: In test_assert_strings, assert b gt c failed. b is "Arclamp" and c is "Bogozity"!
+zutest.c:371: In test_assert_strings, assert b ge a failed. b is "Arclamp" and a is "Bogozity"!
+zutest.c:372: In test_assert_strings, assert c lt a failed. c is "Bogozity" and a is "Bogozity"!
+zutest.c:373: In test_assert_strings, assert c lt b failed. c is "Bogozity" and b is "Arclamp"!
+zutest.c:374: In test_assert_strings, assert a le b failed. a is "Bogozity" and b is "Arclamp"!
+zutest.c:377: In test_assert_strings, assert a is empty failed. a is: Bogozity
+zutest.c:378: In test_assert_strings, assert n is empty failed. n is NULL!
+zutest.c:381: In test_assert_strings, assert e is nonempty failed. e[0] is 0!
+zutest.c:382: In test_assert_strings, assert n is nonempty failed. n is NULL!
+All OK.  11 tests run, 11 successes (195 assertions).

Modified: trunk/zutest.c
===================================================================
--- trunk/zutest.c	2007-01-25 02:30:51 UTC (rev 128)
+++ trunk/zutest.c	2007-01-25 03:14:50 UTC (rev 129)
@@ -48,9 +48,8 @@
 		const char *msg, ...)
 {
 	va_list ap;
-
 	if(!inversion || show_failures) {
-		fprintf(stderr, "FAIL %s at %s line %d:\n\t", func, file, line);
+		fprintf(stderr, "%s:%d: In %s, assert ", file, line, func);
 		va_start(ap, msg);
 		vfprintf(stderr, msg, ap);
 		va_end(ap);
@@ -119,6 +118,13 @@
 }
 
 
+void run_unit_tests_showing_failures(const zutest_suites suites)
+{
+	show_failures = 1;
+	run_unit_tests(suites);
+}
+
+
 /**
  * Examines the command-line arguments.  If "--run-unit-tests" is
  * the first argument, then it runs the unit tests (further arguments
@@ -406,9 +412,11 @@
 	if(argc > 1) {
 		// "zutest -f" prints all the failures in the zutest unit tests.
 		// This allows you to check the output of each macro.
-		show_failures = 1;
+		run_unit_tests_showing_failures(all_zutests);
+	} else {
+		run_unit_tests(all_zutests);
 	}
-	run_unit_tests(all_zutests);
+	// this will never be reached
 	return 0;
 }
 #endif

Modified: trunk/zutest.h
===================================================================
--- trunk/zutest.h	2007-01-25 02:30:51 UTC (rev 128)
+++ trunk/zutest.h	2007-01-25 03:14:50 UTC (rev 129)
@@ -38,6 +38,9 @@
 #ifndef ZUTEST_H
 #define ZUTEST_H
 
+//#define ZUTBECAUSE " failed because "
+#define ZUTBECAUSE " failed. "
+
 // Note that Fail doesn't increment zutest_assertions (the number of assertions
 // that have been made) because it doesn't assert anything.  It only fails.
 // If you call fail, you might want to increment zutest_assertions
@@ -88,9 +91,9 @@
 
 // Pointers...
 #define AssertPtr(p)  AssertFmt(p != NULL, \
-		#p" != NULL but "#p"==0x%lX!", (unsigned long)p)
+		#p" != NULL" ZUTBECAUSE #p"==0x%lX!", (unsigned long)p)
 #define AssertNull(p) AssertFmt(p == NULL, \
-		#p" == NULL but "#p"==0x%lX!", (unsigned long)p)
+		#p" == NULL" ZUTBECAUSE #p"==0x%lX!", (unsigned long)p)
 #define AssertNonNull(p) AssertPtr(p)
 
 #define AssertPtrNull(p) AssertNull(p)
@@ -128,13 +131,13 @@
 
 // ensures a string is non-null but zero-length
 #define AssertStrEmpty(p) do { zutest_assertions++; \
-		if(!(p)) { Fail(#p" should be empty but it is NULL!"); } \
-		if((p)[0]) { Fail(#p" should be empty but it is: %s",p); } \
+		if(!(p)) { Fail(#p" is empty" ZUTBECAUSE #p " is NULL!"); } \
+		if((p)[0]) { Fail(#p" is empty" ZUTBECAUSE #p " is: %s",p); } \
 	} while(0)
 // ensures a string is non-null and non-zero-length
 #define AssertStrNonEmpty(p) do { zutest_assertions++; \
-		if(!(p)) { Fail(#p" should be nonempty but it is NULL!"); } \
-		if(!(p)[0]) { Fail(#p" should be nonempty but "#p"[0] is 0"); } \
+		if(!(p)) { Fail(#p" is nonempty" ZUTBECAUSE #p " is NULL!"); } \
+		if(!(p)[0]) { Fail(#p" is nonempty" ZUTBECAUSE #p"[0] is 0!"); } \
 	} while(0)
 
 
@@ -144,12 +147,12 @@
 //
 
 #define AssertExpType(x,y,op,type,fmt) \
-	AssertFmt((type)x op (type)y, #x" "#op" "#y" failed because " \
+	AssertFmt((type)x op (type)y, #x" "#op" "#y ZUTBECAUSE \
 	#x"=="fmt" and "#y"=="fmt"!", (type)x,(type)y)
 // The failure "x==0 failed because x==1 and 0==0" s too wordy so we'll
 // special-case checking against 0: x==0 failed because x==1).
 #define AssertExpToZero(x,op,type,fmt) \
-	AssertFmt((type)x op 0,#x" "#op" 0 failed because "#x"=="fmt"!", (type)x)
+	AssertFmt((type)x op 0,#x" "#op" 0" ZUTBECAUSE #x"=="fmt"!", (type)x)
 
 #define AssertOp(x,y,op) AssertExpType(x,y,op,long,"%ld")
 #define AssertHexOp(x,y,op) AssertExpType(x,y,op,long,"0x%lX")
@@ -158,7 +161,7 @@
 #define AssertPtrOp(x,y,op) AssertExpType(x,y,op,unsigned long,"0x%lX")
 #define AssertFloatOp(x,y,op) AssertExpType(x,y,op,double,"%lf")
 #define AssertStrOp(x,y,opn,op) AssertFmt(strcmp(x,y) op 0, \
-	#x" "#opn" "#y" but "#x" is \"%s\" and "#y" is \"%s\"!",x,y)
+	#x" "#opn" "#y ZUTBECAUSE #x" is \"%s\" and "#y" is \"%s\"!",x,y)
 
 
 
@@ -249,6 +252,7 @@
  */
 
 void run_unit_tests(const zutest_suites suites);
+void run_unit_tests_showing_failures(const zutest_suites suites);
 
 
 /** Zutest's built-in test suite.





